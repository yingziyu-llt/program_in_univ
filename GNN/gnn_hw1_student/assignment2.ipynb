{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN course assignment 2\n",
    "\n",
    "### This assignment guides you to implement hand-crafted features for node- and link-level tasks on graphs. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingziyu/.conda/envs/pyg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric import transforms as T\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='./data', name='Cora')\n",
    "data = dataset[0]  # Single graph dataset (Cora)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different hand-crafted features below and see how they affect the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import (\n",
    "    degree, \n",
    "    to_networkx, \n",
    "    scatter\n",
    ")\n",
    "\n",
    "### Node-Level Features ###\n",
    "def extract_node_features(data: Data) -> Data:\n",
    "    \"\"\"\n",
    "    Extracts node-level features.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: A PyTorch Geometric data object\n",
    "    \n",
    "    Returns:\n",
    "    - data: A PyTorch Geometric data object with the extracted features\n",
    "    \"\"\"\n",
    "    \n",
    "    # E.g., each node's degree\n",
    "    degrees = degree(data.edge_index[0]).view(-1, 1).float()\n",
    "    # TODO:\n",
    "    # You may replace the original node features with the extracted features\n",
    "    # data.x = degrees\n",
    "    # You may concatenate the extracted features to the original node features\n",
    "    # data.x = torch.cat([data.x, degrees], dim=1)\n",
    "    \n",
    "    # TODO: You may include your hand-crafted features here\n",
    "    out_degrees = degree(data.edge_index[0], num_nodes=data.num_nodes).view(-1, 1).float()\n",
    "    in_degrees = degree(data.edge_index[1], num_nodes=data.num_nodes).view(-1, 1).float()\n",
    "    clustering_coeff = nx.clustering(to_networkx(data)).values()\n",
    "    neighbor_out_degrees = out_degrees[data.edge_index[1]] # 邻居的(出)度\n",
    "    avg_neighbor_degree = scatter(neighbor_out_degrees, data.edge_index[0], dim=0, reduce='mean', dim_size=data.num_nodes)\n",
    "    neighbor_x = data.x[data.edge_index[1]].float()\n",
    "    avg_neighbor_x = scatter(neighbor_x, data.edge_index[0], dim=0, reduce='mean', dim_size=data.num_nodes)\n",
    "    data.x = torch.cat([\n",
    "        data.x,             \n",
    "        in_degrees,\n",
    "        out_degrees,\n",
    "        avg_neighbor_x,\n",
    "    ], dim=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "### Link-Level Features ###\n",
    "def extract_link_features(data: Data, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extracts link-level features for a PyTorch Geometric data object. Concatenates the extracted features to the edge attributes.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: A PyTorch Geometric data object\n",
    "    - edge_index: Target edge indices; a torch.Tensor of shape (2, num_edges)\n",
    "    \n",
    "    Returns:\n",
    "    - edge_attr: edge attributes for the target edges; a torch.Tensor of shape (num_edges, num_features)\n",
    "    \"\"\"\n",
    "    # E.g., the inner product of the source and target node features\n",
    "    inner_product = torch.sum(data.x[edge_index[0]] * data.x[edge_index[1]], dim=1).view(-1, 1).float()\n",
    "    edge_attr = inner_product\n",
    "\n",
    "    # TODO: You may include your hand-crafted features here\n",
    "    src_features = data.x[edge_index[0]]\n",
    "    tgt_features = data.x[edge_index[1]]\n",
    "    # 2. Hadamard 积 (特征交互) [E, F]\n",
    "    hadamard_product = src_features * tgt_features\n",
    "    \n",
    "    # 3. L1 距离 (差异性) [E, F]\n",
    "    l1_diff = torch.abs(src_features - tgt_features)\n",
    "\n",
    "    # 4. 优先连接 (拓扑特征) [E, 1]\n",
    "    # 我们需要计算所有节点的度数（只需要计算一次）\n",
    "    # 假设是无向图，我们使用总度数\n",
    "    # 注意: data.edge_index 是图的 *所有* 边, 而 edge_index 是 *目标* 边\n",
    "    node_degrees = degree(data.edge_index[0], num_nodes=data.num_nodes).float()\n",
    "    \n",
    "    src_degree = node_degrees[edge_index[0]].view(-1, 1)\n",
    "    tgt_degree = node_degrees[edge_index[1]].view(-1, 1)\n",
    "    pref_attachment = src_degree * tgt_degree\n",
    "\n",
    "    # --- 结束 TODO ---\n",
    "\n",
    "    # 将所有特征拼接起来\n",
    "    edge_attr = torch.cat([\n",
    "        inner_product,\n",
    "        hadamard_product,\n",
    "        l1_diff,\n",
    "        pref_attachment\n",
    "    ], dim=1)\n",
    "    assert len(edge_attr) == edge_index.size(1)\n",
    "    return edge_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node-level task: Node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9581257104873657\n",
      "Accuracy: 0.3160\n",
      "Epoch 200, Loss: 0.001546265440993011\n",
      "Accuracy: 0.6960\n",
      "Epoch 400, Loss: 0.0006845730822533369\n",
      "Accuracy: 0.7000\n",
      "Epoch 600, Loss: 0.00039258605102077127\n",
      "Accuracy: 0.6980\n",
      "Epoch 800, Loss: 0.0002568513446021825\n",
      "Accuracy: 0.6980\n",
      "Epoch 1000, Loss: 0.00018178648315370083\n",
      "Accuracy: 0.7000\n",
      "Epoch 1200, Loss: 0.00013552703603636473\n",
      "Accuracy: 0.6980\n",
      "Epoch 1400, Loss: 0.000104812606878113\n",
      "Accuracy: 0.6980\n",
      "Epoch 1600, Loss: 8.328865806106478e-05\n",
      "Accuracy: 0.6980\n",
      "Epoch 1800, Loss: 6.756823131581768e-05\n",
      "Accuracy: 0.6960\n",
      "Test Accuracy: 0.6880\n"
     ]
    }
   ],
   "source": [
    "### A naive Linear Probing Model for node classification ###\n",
    "class LinearProbingModelNode(nn.Module):\n",
    "    task = 'node'\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearProbingModelNode, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        out = self.linear(data.x)\n",
    "        logits = F.log_softmax(out, dim=1)\n",
    "        return logits\n",
    "\n",
    "data = extract_node_features(data) # Extract node-level features\n",
    "\n",
    "model = LinearProbingModelNode(\n",
    "        input_dim=data.x.size(1),\n",
    "        output_dim=dataset.num_classes\n",
    "    )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model\n",
    "model.train()\n",
    "for epoch in range(2000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 200 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        _, pred = model(data).max(dim=1)\n",
    "        correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "        acc = int(correct) / int(data.val_mask.sum())\n",
    "        print(f'Accuracy: {acc:.4f}')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Test Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link-level task: Link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the Cora dataset\n",
    "dataset = Planetoid(root='./data/Cora', name='Cora')\n",
    "data = dataset[0]  # Single graph dataset (Cora)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[2708, 1433], edge_index=[2, 8448], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[8448], edge_label_index=[2, 8448]),\n",
       " Data(x=[2708, 1433], edge_index=[2, 8448], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054]),\n",
       " Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = T.RandomLinkSplit(num_val=0.1, num_test=0.1, is_undirected=data.is_undirected())\n",
    "train_data, val_data, test_data = transform(data)\n",
    "train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A naive Linear Probing Model for link prediction ###\n",
    "class LinearProbingModelLink(nn.Module):\n",
    "    task = 'link'\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=2):\n",
    "        super(LinearProbingModelLink, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, edge_attr):\n",
    "        out = self.linear(edge_attr)\n",
    "        logits = F.log_softmax(out, dim=1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.2076, Val Acc: 0.4839\n",
      "Epoch: 050, Loss: 0.3210, Val Acc: 0.7818\n",
      "Epoch: 100, Loss: 0.2847, Val Acc: 0.7827\n",
      "Epoch: 150, Loss: 0.2681, Val Acc: 0.7761\n",
      "Epoch: 200, Loss: 0.2579, Val Acc: 0.7647\n",
      "Epoch: 250, Loss: 0.2510, Val Acc: 0.7657\n",
      "Epoch: 300, Loss: 0.2460, Val Acc: 0.7647\n",
      "Epoch: 350, Loss: 0.2422, Val Acc: 0.7590\n",
      "Epoch: 400, Loss: 0.2392, Val Acc: 0.7600\n",
      "Epoch: 450, Loss: 0.2368, Val Acc: 0.7600\n",
      "Test Accuracy: 0.7770\n"
     ]
    }
   ],
   "source": [
    "train_data_edge_attr = extract_link_features(train_data, train_data.edge_label_index)\n",
    "val_data_edge_attr = extract_link_features(val_data, val_data.edge_label_index)\n",
    "test_data_edge_attr = extract_link_features(test_data, test_data.edge_label_index)\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearProbingModelLink(\n",
    "    input_dim=train_data_edge_attr.size(1),\n",
    "    output_dim=2\n",
    ")\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(train_data_edge_attr)\n",
    "    labels = train_data.edge_label.long()  # assuming the label is edge_label\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Validation/testing function\n",
    "def test(test_data, test_data_edge_attr):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(test_data_edge_attr)\n",
    "        labels = test_data.edge_label.long()\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct = pred.eq(labels).sum().item()\n",
    "        acc = correct / labels.size(0)\n",
    "    return acc\n",
    "\n",
    "# Training and Evaluation Loop\n",
    "for epoch in range(0, 500):\n",
    "    loss = train()\n",
    "    if epoch % 50 == 0:\n",
    "        val_acc = test(val_data, val_data_edge_attr)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# Test accuracy\n",
    "test_acc = test(test_data, test_data_edge_attr)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于图上的任务，邻接顶点的信息是非常重要的。前面用过扩散小波提取特征，但很失败，严重过拟合。我认为这不是扩散小波变换的问题，而是分类器本身难以处理这种信息，需要建立一个更强的分类器，并加入一系列防止过拟合的措施。  \n",
    "边分类时，源点和终点的信息是至关重要的。通过提取源点和终点信息的相关性，就差不多能做到边的分类。我后面加入的一些手动的特征实际上也是做这个任务，可能有些重复了。因此性能提升很小。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
