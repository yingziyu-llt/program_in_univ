{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Graph Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main question: Given a network with labels on some nodes, how do we assign labels to all other nodes in the network?\n",
    "\n",
    "This notebook guides you through iterative classification for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Import necessary libraries and import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='./data/Cora', name='Cora')\n",
    "\n",
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Implement your classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg\n",
    "# Classifier 1: node features -> node labels\n",
    "class Phi1(torch.nn.Module):\n",
    "    # TODO: Implement the first classifier\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Phi1, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_channels, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            torch.nn.Linear(64, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "# Classifier 2: (node features, labels of neighbors) -> node labels\n",
    "class Phi2(torch.nn.Module):\n",
    "    # TODO: Implement the second classifier\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Phi2, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_channels, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            torch.nn.Linear(64, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Training phase 1 (node features only for Phi 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9471\n",
      "Epoch 100, Loss: 0.1587\n",
      "Epoch 200, Loss: 0.0932\n",
      "Epoch 300, Loss: 0.1096\n",
      "Epoch 400, Loss: 0.0851\n",
      "Epoch 500, Loss: 0.0815\n",
      "Epoch 600, Loss: 0.0845\n",
      "Epoch 700, Loss: 0.0688\n",
      "Epoch 800, Loss: 0.0839\n",
      "Epoch 900, Loss: 0.0939\n",
      "Train accuracy:  1.0\n",
      "Val accuracy:  0.588\n"
     ]
    }
   ],
   "source": [
    "def train_phi(model: torch.nn.Module, x: torch.Tensor, data: Data, optimizer: torch.optim.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Implement the training loop for the first classifier. Note that you should use the training mask to ensure that only training nodes are used.\n",
    "    Args:\n",
    "        model: The classifier to be trained\n",
    "        x: The input features with shape (num_nodes, num_node_features)\n",
    "        data: The graph data\n",
    "        optimizer: The optimizer used for training\n",
    "    Returns:\n",
    "        The loss value\n",
    "    \"\"\"\n",
    "    # TODO: Implement the training loop\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test_phi(model: torch.nn.Module, x: torch.Tensor, data: Data) -> None:\n",
    "    \"\"\"\n",
    "    Implement the testing loop for the first classifier. Note that you should use the training mask to ensure that only training nodes are used.\n",
    "    Args:\n",
    "        model: The classifier to be tested\n",
    "        x: The input features with shape (num_nodes, num_node_features)\n",
    "        data: The graph data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out = model(x)\n",
    "    pred = out.argmax(dim=1)\n",
    "    train_correct = pred[data.train_mask] == data.y[data.train_mask]\n",
    "    train_acc = int(train_correct.sum()) / int(data.train_mask.sum())\n",
    "    val_correct = pred[data.val_mask] == data.y[data.val_mask]\n",
    "    val_acc = int(val_correct.sum()) / int(data.val_mask.sum())\n",
    "    print(\"Train accuracy: \", train_acc)\n",
    "    print(\"Val accuracy: \", val_acc)\n",
    "\n",
    "\n",
    "# TODO: Initialize the first classifier and the optimizer\n",
    "data.x = data.x / data.x.sum(1, keepdim=True).clamp(min=1)\n",
    "phi1 = Phi1(dataset.num_node_features, dataset.num_classes)\n",
    "optimizer1 = torch.optim.Adam(phi1.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss = train_phi(phi1, data.x, data, optimizer1)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "test_phi(phi1, data.x, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Training phase 1 (node features and relational features for Phi 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_relational_features(data: Data, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Summarize the relational features of each node in the graph\n",
    "    Args:\n",
    "        data: The graph data\n",
    "        y_pred: The predicted labels of the nodes with shape (num_nodes); note that y_pred[data.train_mask] is the ground truth labels\n",
    "    Returns:\n",
    "        The relational features with shape (num_nodes, num_relational_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the function\n",
    "    num_nodes = data.num_nodes\n",
    "    num_classes = dataset.num_classes\n",
    "    y_onehot = F.one_hot(y_pred, num_classes).float()\n",
    "    neighbor_summary = torch.zeros(num_nodes, num_classes, device=data.x.device)\n",
    "    \n",
    "    src, dst = data.edge_index\n",
    "    neighbor_summary.index_add_(0, dst, y_onehot[src])\n",
    "    \n",
    "    degree = torch.zeros(num_nodes, 1, device=data.x.device)\n",
    "    degree.index_add_(0, dst, torch.ones(src.size(0), 1, device=data.x.device))\n",
    "    degree = degree.clamp(min=1)\n",
    "    neighbor_summary = neighbor_summary / degree\n",
    "    \n",
    "    return torch.cat([data.x, neighbor_summary], dim=1)\n",
    "\n",
    "def get_y_pred(x: torch.Tensor, data: Data, model: torch.nn.Module) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the predicted labels of the nodes; set the labels of training nodes to the ground truth labels.\n",
    "    Args:\n",
    "        x: The input features with shape (num_nodes, num_node_features)\n",
    "        data: The graph data\n",
    "        model: The classifier\n",
    "    Returns:\n",
    "        The predicted labels with shape (num_nodes)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out = model(x)\n",
    "    y_pred = out.argmax(dim=1)\n",
    "    y_pred[data.train_mask] = data.y[data.train_mask] # fix the labels of training nodes\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9500\n",
      "Epoch 100, Loss: 0.0208\n",
      "Epoch 200, Loss: 0.0078\n",
      "Epoch 300, Loss: 0.0050\n",
      "Epoch 400, Loss: 0.0031\n",
      "Epoch 500, Loss: 0.0037\n",
      "Epoch 600, Loss: 0.0025\n",
      "Epoch 700, Loss: 0.0018\n",
      "Epoch 800, Loss: 0.0023\n",
      "Epoch 900, Loss: 0.0008\n",
      "Train accuracy:  1.0\n",
      "Val accuracy:  0.642\n"
     ]
    }
   ],
   "source": [
    "# TODO: Initialize the second classifier and the optimizer\n",
    "phi2 = Phi2(dataset.num_features + dataset.num_classes, dataset.num_classes)\n",
    "optimizer2 = torch.optim.Adam(phi2.parameters(), lr=0.01)\n",
    "\n",
    "# Initial training with predicted labels from phi1\n",
    "y_pred = get_y_pred(data.x, data, phi1)\n",
    "# Concatenate the relational features to the node features\n",
    "xz = cat_relational_features(data, y_pred)\n",
    "for epoch in range(1000):\n",
    "    loss = train_phi(phi2, xz, data, optimizer2)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "test_phi(phi2, xz, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Phase 2; iterate until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Iterative Classification ---\n",
      "Iter 1: Val Acc: 0.6420, Labels Changed: 981\n",
      "Iter 2: Val Acc: 0.7040, Labels Changed: 561\n",
      "Iter 3: Val Acc: 0.6740, Labels Changed: 394\n",
      "Iter 4: Val Acc: 0.6920, Labels Changed: 338\n",
      "Iter 5: Val Acc: 0.6800, Labels Changed: 315\n",
      "Iter 6: Val Acc: 0.6920, Labels Changed: 299\n",
      "Iter 7: Val Acc: 0.6820, Labels Changed: 284\n",
      "Iter 8: Val Acc: 0.6940, Labels Changed: 276\n",
      "Iter 9: Val Acc: 0.6820, Labels Changed: 275\n",
      "Iter 10: Val Acc: 0.6940, Labels Changed: 275\n",
      "Final Test Accuracy: 0.7090\n"
     ]
    }
   ],
   "source": [
    "def iterative_classification(data: Data, phi1: torch.nn.Module, phi2: torch.nn.Module, max_iters: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    Implement the iterative classification algorithm (phase 2).\n",
    "    Args:\n",
    "        data: The graph data\n",
    "        phi1: The first classifier\n",
    "        phi2: The second classifier\n",
    "        max_iters: The maximum number of iterations\n",
    "    \"\"\"\n",
    "    y_pred = get_y_pred(data.x, data, phi1)\n",
    "    prev_y_pred = y_pred.clone()\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        # TODO: Implement the iterative classification algorithm (phase 2)\n",
    "        xz = cat_relational_features(data, y_pred)   \n",
    "        with torch.no_grad():\n",
    "            out = phi2(xz)\n",
    "            new_y_pred = out.argmax(dim=1)\n",
    "        new_y_pred[data.train_mask] = data.y[data.train_mask]\n",
    "        change_count = (new_y_pred != y_pred).sum().item()\n",
    "        val_correct = new_y_pred[data.val_mask] == data.y[data.val_mask]\n",
    "        val_acc = int(val_correct.sum()) / int(data.val_mask.sum())\n",
    "        print(f'Iter {i+1}: Val Acc: {val_acc:.4f}, Labels Changed: {change_count}')\n",
    "        y_pred = new_y_pred\n",
    "        \n",
    "        if change_count == 0:\n",
    "            print(\"Converged!\")\n",
    "            break\n",
    "    acc = (y_pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()\n",
    "    print(f'Test Accuracy: {acc:.4f}')\n",
    "\n",
    "\n",
    "iterative_classification(data, phi1, phi2,max_iters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个任务揭示了一个很有趣的现象。单点预测确实肯定是效果很差的，但是用这个差的效果经过网络的迭代，居然最后能获得一个还不错的结果。rubbish in, rubbish out的规律似乎不成立了。但是，其实我们可以观察到，这个迭代的过程中不是简单的rubbish in的过程，而是加入了高质量的网络信息。可能正是这种网络信息提升了模型的表现。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
