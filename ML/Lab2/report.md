# 机器学习概论 课程项目2

## 1\. 引言 (Introduction)

自动对对联是一项极具挑战性的自然语言生成任务，要求输出在字数、词性、平仄和意境上与输入高度匹配。原有的基线系统主要基于序列标注（Sequence Labeling），受限于定长对齐的假设。为了实现更通用的生成能力，本项目首先重构为基础的 **Seq2Seq (Encoder-Decoder)** 自回归框架，随后针对实验中发现的“长句遗忘”和“重复生成”问题，进一步引入 **Bahdanau Attention** 机制与 **双向 GRU**，实现了生成质量的显著提升。

## 2\. 第一阶段：基础 Seq2Seq 模型的构建与瓶颈

### 2.1 模型架构 (Model Architecture)

在实验的第一阶段，我们构建了一个标准的 Encoder-Decoder 模型，旨在验证自回归生成的可行性。

  * **代码实现**：
      * **Encoder & Decoder**：均采用单向 `nn.GRU`。
      * **变长处理**：在 `forward` 函数中引入了 `src_len` 参数，使用 `pack_padded_sequence` 对输入进行打包，以屏蔽 Padding 对隐藏状态的干扰。
      * **交互方式**：Encoder 将整个上联压缩为一个最终的 Hidden State，直接作为 Decoder 的初始状态。
      * **超参数**：batchsize=64,embedded_dim=128,n_layer=2,hidden_dim=256

### 2.2 实验结果与问题分析
原始结果：
```
2025-12-10 04:02:40,760 - INFO - epoch time: 740.54, accumulation loss: 50171.7
2025-12-10 04:02:40,792 - INFO - 上联：马齿草焉无马齿。 预测的下联：羊皮狗也有鸡毛
2025-12-10 04:02:40,797 - INFO - 上联：天古天今，地中地外，古今中外存天地。 预测的下联：人雄人杰，人外人间，古今古今论古今
2025-12-10 04:02:40,803 - INFO - 上联：笑取琴书温旧梦。 预测的下联：闲听竹竹赋新词
2025-12-10 04:02:40,808 - INFO - 上联：日里千人拱手划船，齐歌狂吼川江号子。 预测的下联：天间一里同心入我，尽醉同心天地春风
2025-12-10 04:02:40,814 - INFO - 上联：我有诗情堪纵酒。 预测的下联：谁无意意可吟诗
2025-12-10 04:02:40,819 - INFO - 上联：我以真诚溶冷血。 预测的下联：人因大爱铸清心
2025-12-10 04:02:40,824 - INFO - 上联：三世业岐黄，妙手回春人共赞。 预测的下联：一年歌盛世，和风逐梦梦同圆
2025-12-10 04:02:42,284 - INFO - BLEU: 0.032696589, Rouge-L: 0.15364386
```
  * **定量指标**：
      * **BLEU**: 0.0327
      * **Rouge-L**: 0.1536
      * **Loss**: 50171.7
  * **定性分析**：
    虽然模型跑通了流程，但在生成结果中暴露了 **Seq2Seq 模型的典型缺陷**——定长向量无法承载复杂长句的全部信息，导致 Decoder 在解码过程中“丢失记忆”，并陷入局部循环。
      * **复读机现象**：
          * 上联：`笑取琴书温旧梦`
          * 下联：`闲听竹竹赋新词` （出现了“竹竹”的重复，且“听”与“取”对仗不工）
          * 上联：`天古天今...` -\> 下联：`...古今古今论古今`
      * **长句语义崩坏**：
          * 上联：`日里千人拱手划船...`
          * 下联：`天间一里同心入我...` （后半句逻辑完全混乱，意境丢失）

## 3\. 第二阶段：引入 Attention 与双向编码的架构升级

### 3.1 架构改进 (Architecture Improvements)

为了解决上述问题，我们对模型进行了深度的架构重构。

  * **双向编码 (Bidirectional Encoder)**：
      * 将 Encoder 升级为 **双向 GRU** (`bidirectional=True`)。这使得每个字都能同时感知前文和后文的信息，语义表示更加丰富。
  * **注意力机制 (Bahdanau Attention)**：
      * 引入 `Attention` 模块，计算 Decoder 当前状态与 Encoder 所有时刻输出 `encoder_outputs` 的对齐权重（Energy）。
      * **Masking 策略**：在计算 Attention 时显式地对 Pad 位置进行 Mask (`masked_fill(mask == 0, -1e10)`)，防止注意力分散到无效区域。
      * **上下文融合**：计算出的 Context Vector 与当前输入的 Embedding 拼接后送入 Decoder RNN。

### 3.2 训练动态调整

在模型复杂度提升后，为了防止梯度爆炸（曾导致 Loss 飙升），我们在训练策略上同步进行了调整：

  * **学习率**：降低至 `3e-4`。
  * **梯度裁剪**：收紧至 `1.0`。

## 4\. 最终实验结果 (Final Results)

### 4.1 定量提升

  * **BLEU**: **0.0366** (提升约 12%)
  * **Rouge-L**: **0.1625** (提升约 6%)
  * **Accumulation Loss**: **47224.1** (显著降低)

### 4.2 生成质量对比 (Qualitative Comparison)

引入 Attention 后，模型展现了极强的对齐能力，彻底解决了第一阶段的“复读机”和“长句遗忘”问题。

| 上联案例 | 基础 Seq2Seq (Phase 1) | **Attention Seq2Seq (Phase 2)** | 点评 |
| :--- | :--- | :--- | :--- |
| **笑取琴书温旧梦** | 闲听**竹竹**赋新词 | **闲将笔墨写新情** | 修复了“竹竹”叠词；“琴书”对“笔墨”、“旧梦”对“新情”，对仗极其工整。 |
| **我以真诚溶冷血** | 人因大爱铸清心 | **谁凭大义凛冰心** | 两个模型都生成得不错，但 Phase 2 的“凛冰心”在意境上更胜一筹，“溶”与“凛”动词对仗更精准。 |
| **日里千人拱手划船...** | 天间一里同心入我... | **天中万众挥毫挥笔...** | 面对长难句，Phase 2 成功保持了全句的结构稳定，“千人”对“万众”，且没有出现逻辑崩坏。 |

cli demo：
```
上联：人如芳草春常在
下联：心似清花梦自高
上联：长空万里
下联：大地一轮
上联：直上青天揽白月
下联：不来碧水洗红尘
上联：大江东去
下联：小月西来
上联：春风又绿江南岸
下联：旭日初红塞北天
```
## 5\. 结论 (Conclusion)

本实验通过对比验证，清晰地展示了自动对联模型从“死记硬背”到“理解创作”的进化过程。

1.  **基础 Seq2Seq** 虽然实现了变长生成，但受限于信息压缩瓶颈，容易产生重复和语义丢失。
2.  **Attention 机制** 赋予了模型在解码时“回看”上联的能力，配合 **双向 Encoder** 对语义的深度捕捉，使得生成的下联在逻辑性、对仗工整度和多样性上均实现了质的飞跃。