{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多模态学习第一次作业：神经网络基础\n",
    "\n",
    "### 说明：本次作业满分100分。共6小题，其中前5小题每题10分，第6小题50分。\n",
    "\n",
    "### 本次作业中，你需要补全神经网络中基础模块的前向计算方法，并使用这些模块，构建一个简单的神经网络模型，完成一个分类任务。\n",
    "\n",
    "### **代码填空要求**:\n",
    "- **在有说明的情况下，可以使用torch.nn.functional，否则请直接使用torch基本张量运算(如torch.max,torch.sum,torch.matmul等)，不允许使用torch.nn.Module！**\n",
    "- **如必要，代码填空处可以写多行代码，但不得更改其他代码！**\n",
    "\n",
    "### **最终结果正确不代表得分为满分。请仔细阅读题目中说明！**\n",
    "\n",
    "### 提交方式:只需要提交这个ipynb文件，命名为 名字_学号_multimodal_hw01.ipynb。提交文件需要保留运行的结果。\n",
    "\n",
    "### 本次作业不接受任何形式的补交，请仔细阅读关于作业提交的说明！\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **题目1:ReLU**\n",
    "### **描述**\n",
    "补全ReLU类中的前向计算，对输入张量`x`进行ReLU激活，返回输出张量`y`。\n",
    "\n",
    "### **示例输入**\n",
    "\n",
    "```python\n",
    "x = torch.tensor([\n",
    "    [\n",
    "        [[1, -1], [3, 4]],\n",
    "        [[-5, 6], [-9, 8]]\n",
    "    ]\n",
    "])  # 形状为 (1, 2, 2, 2)\n",
    "```\n",
    "\n",
    "### **示例输出**\n",
    "\n",
    "```python\n",
    "y = torch.tensor([\n",
    "    [\n",
    "        [[1, 0], [3, 4]],\n",
    "        [[0, 6], [0, 8]]\n",
    "    ]\n",
    "])  # 形状为 (1, 2, 2, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播: f(x) = max(0, x)\n",
    "        :param x: 输入 torch.Tensor\n",
    "        :return: 激活后的输出 (torch.Tensor)\n",
    "        \"\"\"\n",
    "        # 记录哪些位置小于等于0\n",
    "        self.mask = (x <= 0)\n",
    "\n",
    "        ## TODO;\n",
    "        y = x.clone()\n",
    "        y[self.mask] = 0\n",
    "        return y\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向传播: 只有输入大于0的位置才保留梯度\n",
    "        :param dout: 上一层传来的梯度 (torch.Tensor)\n",
    "        :return: dx (torch.Tensor)\n",
    "        \"\"\"\n",
    "        dx = dout.clone()\n",
    "        dx[self.mask] = 0\n",
    "        return dx\n",
    "\n",
    "\n",
    "module=ReLU()\n",
    "x = torch.tensor([\n",
    "    [\n",
    "        [[1, -1], [3, 4]],\n",
    "        [[-5, 6], [-9, 8]]\n",
    "    ]\n",
    "])  \n",
    "\n",
    "y=module.forward(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **题目2：GlobalMaxPooling**\n",
    "\n",
    "### **描述**\n",
    "\n",
    "补全GlobalMaxPooling类中的前向计算。输入给定一个形状为 `(N, C, H, W)` 的四维张量 `x`，其中 N 为样本数量，C 为通道数，H 和 W 为高和宽。请对 `x` 进行处理，实现以下功能：对每个样本的每个通道进行全局最大池化，得到一个形状为 `(N, C)` 的张量 `y`。\n",
    "\n",
    "### **示例输入**\n",
    "\n",
    "```python\n",
    "x = torch.tensor([\n",
    "    [\n",
    "        [[1, 2], [3, 4]],\n",
    "        [[5, 6], [7, 8]]\n",
    "    ],\n",
    "    [\n",
    "        [[9, 10], [11, 12]],\n",
    "        [[13, 14], [15, 16]]\n",
    "    ]\n",
    "])  # 形状为 (2, 2, 2, 2)\n",
    "```\n",
    "\n",
    "### **示例输出**\n",
    "\n",
    "```python\n",
    "y = torch.tensor([\n",
    "    [4, 8],\n",
    "    [12, 16]\n",
    "])  # 形状为 (2, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GlobalMaxPooling:\n",
    "    def __init__(self):\n",
    "        self.x = None       \n",
    "        self.argmax = None \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向计算: 对输入 x (N, C, H, W) 做全局最大池化，得到 (N, C)\n",
    "        :param x: torch.Tensor\n",
    "        :return: y (N, C)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "\n",
    "        # TODO:\n",
    "        # (N, C, H, W) → (N, C)\n",
    "        y = x.max(dim=3).values.max(dim=2).values\n",
    "\n",
    "\n",
    "\n",
    "        # 记录最大值的位置，用于反向传播\n",
    "        self.argmax = (x == y[:, :, None, None])\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向传播: 将梯度传回到最大值的位置\n",
    "        :param dout: 来自上一层的梯度 (N, C)\n",
    "        :return: dx 与输入同形状 (N, C, H, W)\n",
    "        \"\"\"\n",
    "        N, C, H, W = self.x.shape\n",
    "        dx = torch.zeros_like(self.x)\n",
    "\n",
    "        # dout (N, C) → 广播到 (N, C, H, W)，再只在最大值位置赋值\n",
    "        dx[self.argmax] = dout[:, :, None, None][self.argmax]\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "module=GlobalMaxPooling()\n",
    "x = torch.tensor([\n",
    "    [\n",
    "        [[1, 2], [3, 4]],\n",
    "        [[5, 6], [7, 8]]\n",
    "    ],\n",
    "    [\n",
    "        [[9, 10], [11, 12]],\n",
    "        [[13, 14], [15, 16]]\n",
    "    ]\n",
    "])  # 形状为 (2, 2, 2, 2)\n",
    "y=module.forward(x) #形状为 (2, 2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **题目3：Linear**\n",
    "\n",
    "### **描述**\n",
    "补全 Linear 类中的前向计算。输入给定一个形状为 (N, d_in) 的二维张量 `x`，其中 N 为样本数量，d_in 为输入特征维度。请对 x 进行处理，实现以下功能：通过线性变换 `y = xW + b` 得到一个形状为 `(N, d_out)` 的张量 `y`。\n",
    "\n",
    "### **注意**: 请使用torch中的函数手动实现张量运算，不要直接使用torch.nn.Linear类！\n",
    "\n",
    "### **示例输入**\n",
    "\n",
    "```python\n",
    "x = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0]\n",
    "]) # 形状为 (2, 2)\n",
    "\n",
    "W = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0]\n",
    "])  # 形状为 (2, 2)\n",
    "\n",
    "b = torch.tensor([1.0, -1.0]) #形状为 (2,)\n",
    "```\n",
    "\n",
    "### **示例输出**\n",
    "\n",
    "```python\n",
    "y = torch.tensor([\n",
    "    [ 8., 9.],\n",
    "    [16., 21.]\n",
    "])\n",
    "#   # 形状为 (2, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, d_in, d_out):\n",
    "        # 初始化权重和偏置\n",
    "        self.W = torch.randn(d_in, d_out)\n",
    "        self.b = torch.zeros(d_out)\n",
    "\n",
    "        # 用于保存前向传播时的输入\n",
    "        self.x = None\n",
    "        self.dW=None\n",
    "        self.db=0\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播: y = xW + b\n",
    "        :param x: 输入张量，形状 (N, d_in)\n",
    "        :return: 输出张量，形状 (N, d_out)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        ## TODO:\n",
    "        y = x @ self.W + self.b  # (N, d_out)\n",
    "        return y\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向传播: 计算梯度并更新参数\n",
    "        :param dout: 来自上一层的梯度，形状 (N, d_out)\n",
    "        :param lr: 学习率\n",
    "        :return: dx, 传回前一层的梯度，形状 (N, d_in)\n",
    "        \"\"\"\n",
    "        # 计算梯度\n",
    "        dW =  self.x.T@dout             # (d_in, d_out)\n",
    "        db = dout.sum(dim=0)             # (d_out,)\n",
    "        dx = dout @ self.W.T               # (N, d_in)\n",
    "\n",
    "        self.dW=dW\n",
    "        self.db=db\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "module=Linear(2,2)\n",
    "x = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0]\n",
    "]) # 形状为 (2, 2)\n",
    "\n",
    "module.W = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0]\n",
    "])  # 形状为 (2, 2)\n",
    "\n",
    "module.b= torch.tensor([1.0, -1.0]) #形状为 (2,)\n",
    "\n",
    "y = module.forward(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **题目4：BatchNorm**\n",
    "\n",
    "### **描述**\n",
    "\n",
    "补全BatchNorm类中的前向计算,给定一个形状为 `(N, D)` 的张量 `x`，**不使用** `torch.nn.BatchNorm1d`，计算归一化后的输出 `y`。\n",
    "\n",
    "### **示例输入**\n",
    "\n",
    "```python\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "gamma = torch.tensor([1.0, 1.0])\n",
    "beta = torch.tensor([0.0, 0.0])\n",
    "```\n",
    "\n",
    "### **示例输出**\n",
    "\n",
    "```python\n",
    "y = torch.tensor([[-1.0, -1.0],\n",
    "                  [1.0, 1.0]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        # 可学习参数 gamma 和 beta\n",
    "        self.gamma = torch.ones(num_features)\n",
    "        self.beta = torch.zeros(num_features)\n",
    "\n",
    "        # 存储中间结果\n",
    "        self.eps = eps\n",
    "        self.cache = None\n",
    "        self.dgamma=None\n",
    "        self.dbeta=None\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :param x: 输入张量 (N, D)\n",
    "        :return: 输出张量 y (N, D)\n",
    "        \"\"\"\n",
    "\n",
    "        ## TODO\n",
    "        mean = x.mean(dim=0)   # 均值\n",
    "        var = x.var(dim=0, unbiased=False)\n",
    "        std = torch.sqrt(var + self.eps)\n",
    "        x_n = (x - mean) / (std + self.eps)  # 归一化\n",
    "        y = self.gamma * x_n + self.beta\n",
    "\n",
    "        # 保存中间变量以备反向传播\n",
    "        self.cache = (x, x_n, mean, std)\n",
    "        return y\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param dout: 上一层梯度 (N, D)\n",
    "        :return: dx, dgamma, dbeta\n",
    "        \"\"\"\n",
    "        x, x_n, mean, std = self.cache\n",
    "        N, D = x.shape\n",
    "\n",
    "        # 对 gamma 和 beta 的梯度\n",
    "        dgamma = torch.sum(dout * x_n, dim=0)\n",
    "        dbeta = torch.sum(dout, dim=0)\n",
    "\n",
    "        # 对输入的梯度\n",
    "        dx_n = dout * self.gamma\n",
    "        dx = dx_n / (std + self.eps)   # 简化计算\n",
    "        self.dgamma=dgamma\n",
    "        self.dbeta=dbeta\n",
    "        return dx\n",
    "\n",
    "\n",
    "module=BatchNorm(2)\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "gamma = torch.tensor([1.0, 1.0])\n",
    "beta = torch.tensor([0.0, 0.0])\n",
    "\n",
    "module.gamma=gamma\n",
    "module.beta=beta\n",
    "\n",
    "y = module.forward(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **题目5：Convolution**\n",
    "\n",
    "### **描述**\n",
    "补全 Conv2d 类中的前向计算。输入给定一个形状为 (N, C_in, H, W) 的四维张量 x，其中 N 为样本数量，C_in 为输入通道数，H 和 W 为高和宽。请对 x 进行卷积运算，得到一个形状为 (N, C_out, H_out, W_out) 的输出张量 y。\n",
    "\n",
    "### **注意**:\n",
    "    weight 是卷积核参数，形状为 (C_out, C_in, K, K)\n",
    "\n",
    "    bias 是偏置参数，形状为 (C_out,)\n",
    "\n",
    "    stride 为步长，padding 为填充\n",
    "\n",
    "    卷积计算可以使用 torch.nn.functional.conv2d 完成，但不要使用 torch.nn.Conv2d。\n",
    "\n",
    "### **示例输入**\n",
    "\n",
    "```python\n",
    "x = torch.tensor([[\n",
    "    [[1., 2., 3.],\n",
    "     [4., 5., 6.],\n",
    "     [7., 8., 9.]]\n",
    "]])  # 形状为 (1, 1, 3, 3)\n",
    "\n",
    "weight = torch.tensor([[\n",
    "    [[1., 0.],\n",
    "     [0., -1.]]\n",
    "]])  # 形状为 (1, 1, 2, 2)\n",
    "\n",
    "bias = torch.tensor([0.])  # 形状为 (1,)\n",
    "\n",
    "stride=1, padding=0\n",
    "```\n",
    "\n",
    "### **示例输出**\n",
    "\n",
    "```python\n",
    "y = torch.tensor([[\n",
    "    [[-4., -4.],\n",
    "     [-4., -4.]]\n",
    "]])\n",
    "#   # 形状为 (1,1,2, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, lr=0.01):\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.lr = lr\n",
    "\n",
    "        # 初始化卷积核和偏置\n",
    "        self.weight = torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.bias = torch.zeros(out_channels)\n",
    "\n",
    "        # 存储前向传播的输入\n",
    "        self.x = None\n",
    "        self.dW=None\n",
    "        self.db=None\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播: y = conv2d(x, weight, bias)\n",
    "        :param x: 输入张量 (N, C_in, H, W)\n",
    "        :return: 输出张量 (N, C_out, H_out, W_out)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        ##TODO\n",
    "        y = F.conv2d(x, self.weight, self.bias, stride=self.stride, padding=self.padding)\n",
    "        return y\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # dx: w.r.t 输入\n",
    "        dx = F.conv_transpose2d(dout, self.weight, stride=self.stride, padding=self.padding)\n",
    "\n",
    "        # dW: w.r.t 权重\n",
    "        dW = torch.nn.grad.conv2d_weight(\n",
    "            self.x, self.weight.shape, dout,\n",
    "            stride=self.stride, padding=self.padding\n",
    "        )\n",
    "\n",
    "        # db: w.r.t 偏置\n",
    "        db = dout.sum(dim=(0, 2, 3))\n",
    "\n",
    "        self.dW = dW\n",
    "        self.db = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "module=Conv2d(1,1,2,1,0)\n",
    "x = torch.tensor([[\n",
    "    [[1., 2., 3.],\n",
    "     [4., 5., 6.],\n",
    "     [7., 8., 9.]]\n",
    "]])  # 形状为 (1, 1, 3, 3)\n",
    "\n",
    "module.weight = torch.tensor([[\n",
    "    [[1., 0.],\n",
    "     [0., -1.]]\n",
    "]])  # 形状为 (1, 1, 2, 2)\n",
    "\n",
    "module.bias = torch.tensor([0.])  # 形状为 (1,)\n",
    "\n",
    "y=module.forward(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探究 Depth-wise Separable卷积\n",
    "![image.png](attachment.png)\n",
    "#### **请你回顾Depth-wise Separable卷积的知识，并回答下列问题**:\n",
    "假设不考虑Bias项，有如下输入和常规卷积核：\n",
    "```python\n",
    "x = torch.ones(2, 2, 3, 3)\n",
    "regular_kernel=torch.randn(3, 2, 2, 2)\n",
    "```\n",
    "\n",
    "为了使用Depth-wise Separable卷积替代上述常规卷积，需要两个卷积核,分别实现Depth-wise和Point-wise卷积，如下：\n",
    "```python\n",
    "depth_wise_kernel = torch.randn(a, b, c, d)  \n",
    "point_wise_kernel = torch.randn(e, f, g, h) \n",
    "```\n",
    "\n",
    "为了保证结果形状匹配，请你在下方写出*a*~*h*的值，同时简要说明Depth-wise Separable卷积的优势:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **你的回答:**\n",
    "\n",
    "a=2,b=1,c=3,d=3\n",
    "e=3,f=2,g=1,h=1\n",
    "\n",
    "深度可分离卷积有效降低了总参数量和计算量，将in channel和cross channel分别计算，大幅度提升了效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **题目6：神经网络初探**\n",
    "## 本部分共计50分。\n",
    "\n",
    "### Step 1: 数据加载与处理\n",
    "在这个练习中，我们为你提供了一个经典的图片数据集:MNIST，你需要阅读[这个教程](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)及相关资料，了解Dataloader的相关内容，并找到适合读取MNIST的Dataloader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## Download MNIST\n",
    "train_set = MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_set =MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Check integrity\n",
    "image, label = train_set[0]\n",
    "print(image.shape)   # 输出：28x28 图像\n",
    "print(label)        # 输出：标签，如 5\n",
    "print(len(train_set)) # 输出：60000\n",
    "print(len(test_set))  # 输出：10000\n",
    "\n",
    "## TODO: Dataloader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"Data loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: 构建神经网络模型\n",
    "你需要使用**你先前完成的ReLU类和Linear类**，实现一个3层的MLP模型（一个输入层，一个带有ReLU激活函数的隐藏层，和一个输出层）。下面的代码已经给出框架，请你补全代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        \"\"\"\n",
    "        初始化 MLP 模型\n",
    "        :param d_in: 输入维度\n",
    "        :param d_hidden: 隐藏层维度\n",
    "        :param d_out: 输出维度\n",
    "        \"\"\"\n",
    "\n",
    "        ## TODO:\n",
    "        self.fc1 = Linear(d_in, d_hidden)\n",
    "        self.relu = ReLU()\n",
    "        self.fc2 = Linear(d_hidden, d_out)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :param x: 输入图片 (N, C, H, W)\n",
    "        :return: 输出张量 (N, d_out)\n",
    "        \"\"\"\n",
    "       \n",
    "        ## TODO:\n",
    "        x = x.view(x.size(0), -1)  # 展平 (N, C*H*W)\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param dout: 来自上一层的梯度 (N, d_out)\n",
    "        :param lr: 学习率\n",
    "        :return: None\n",
    "        hint: dout=module.backward(dout)，此时module自身已经储存梯度。\n",
    "        \"\"\"\n",
    "\n",
    "        ## TODO:\n",
    "        dout = self.fc2.backward(dout)\n",
    "        dout = self.relu.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "\n",
    "    def update(self,lr=0.01):\n",
    "        \"\"\"\n",
    "        参数更新\n",
    "        ReLU层无需更新\n",
    "        Linear层：W -= lr*dW, B-=lr*dB\n",
    "        \"\"\"\n",
    "        self.fc1.W -= lr * self.fc1.dW\n",
    "        self.fc1.b -= lr * self.fc1.db\n",
    "        self.fc2.W -= lr * self.fc2.dW\n",
    "        self.fc2.b -= lr * self.fc2.db\n",
    "        ## TODO:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: 交叉熵损失\n",
    "分类问题中通常使用CrossEntropy Loss作为损失函数，计算损失并返回上游梯度。这部分的代码如下，已经实现完毕。请你阅读代码并熟悉用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.logits = None\n",
    "        self.targets = None\n",
    "        self.probs = None\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        前向计算交叉熵\n",
    "        :param logits: [batch_size, num_classes]\n",
    "        :param targets: [batch_size] (类别索引)\n",
    "        :return: scalar loss\n",
    "        \"\"\"\n",
    "        self.logits = logits\n",
    "        self.targets = targets\n",
    "        # 计算 log_softmax\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        # 保存 softmax 概率用于反向传播\n",
    "        self.probs = log_probs.exp()\n",
    "        # 负对数似然损失\n",
    "        loss = F.nll_loss(log_probs, targets, reduction='mean')\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        反向传播，计算 dL/dlogits\n",
    "        :return: [batch_size, num_classes] 的梯度\n",
    "        \"\"\"\n",
    "        batch_size = self.logits.size(0)\n",
    "        grad = self.probs.clone()  # softmax 概率\n",
    "        grad[range(batch_size), self.targets] -= 1  # p - y\n",
    "        grad = grad / batch_size  # 平均\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: 训练与测试\n",
    "请你完成训练函数和测试函数的代码，进行可视化，展示：**训练过程的Loss曲线，训练前后模型在测试集上的准确率。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def train(model, train_loader, lr, epochs):\n",
    "    ## TODO:\n",
    "    loss_fn = CrossEntropy()\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        avg_loss = 0\n",
    "        for step, (img, label) in enumerate(progress_bar):\n",
    "\n",
    "            ## TODO:\n",
    "\n",
    "            output = model.forward(img)\n",
    "            loss = loss_fn.forward(output, label)\n",
    "            grad = loss_fn.backward()  # 上游梯度\n",
    "\n",
    "            # 模型梯度反传与参数更新：\n",
    "            model.backward(grad)\n",
    "            model.update(lr)\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # 每 10 step 更新一次进度条显示\n",
    "            if (step + 1) % 10 == 0:\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = avg_loss / len(train_loader)\n",
    "        print(f\"Average Loss: {epoch_loss:.3f}\")\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "    # 绘制 loss 曲线\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, epochs + 1), losses, marker='o')\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 测试时不需要梯度\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            output = model.forward(img)           # [batch_size, num_classes]\n",
    "            preds = output.argmax(dim=1)          # 取最大概率的类别\n",
    "            correct += (preds == label).sum().item()\n",
    "            total += label.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "## 模型训练预测试\n",
    "\n",
    "model=MLP(28*28,128,10) # 28*28是图片尺寸\n",
    "lr=0.005 # 学习率可自行调整\n",
    "\n",
    "test(model,test_loader) # 训练前测试\n",
    "train(model,train_loader,lr,epochs=10)  # 训练10epochs\n",
    "test(model,test_loader) # 训练后测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: 探索超参数\n",
    "训练过程中涉及的超参数包括:\n",
    "- batch_size: 批次大小\n",
    "- learning_rate: 学习率\n",
    "- d_hidden: 隐藏层维度\n",
    "  \n",
    "请你从中选出1~3个，控制变量实验，探究超参数的改变对结果的影响，并说明除了超参数设计之外，还有哪些可以提升模型性能的方法？在下面进行结果展示和解释说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调一下lr\n",
    "model=MLP(28*28,128,10) # 28*28是图片尺寸\n",
    "lr=0.01 # 学习率可自行调整\n",
    "\n",
    "test(model,test_loader) # 训练前测试\n",
    "train(model,train_loader,lr,epochs=10)  # 训练10epochs\n",
    "test(model,test_loader) # 训练后测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调一下hidden size\n",
    "model=MLP(28*28,256,10) # 28*28是图片尺寸\n",
    "lr=0.005 # 学习率可自行调整\n",
    "\n",
    "test(model,test_loader) # 训练前测试\n",
    "train(model,train_loader,lr,epochs=10)  # 训练10epochs\n",
    "test(model,test_loader) # 训练后测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **你的回答:**\n",
    "可以看到，加大lr和hidden size,都会提升模型性能。这证明了，模型处于一定的欠拟合状态，且容量也不够大。除了调朝参以外，还可以做LR退火等方法，进一步提升性能。更加直接的方法是改变模型结构为CNN,一般来说可以把Acc提升到96%以上。此外，还可以做数据增强并加大训练轮数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
