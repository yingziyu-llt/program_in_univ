{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: NLP Basics\n",
    "In this assignment, you will implement and explore three fundamental concepts in Natural Language Processing: word embeddings (GloVe), subword tokenization (BPE), and recurrent neural networks for text generation (LSTM). This will give you a hands-on understanding of how text is represented and processed in modern NLP models.\n",
    "\n",
    "**Due date: 2025.11.16 , 23:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup and Data Download\n",
    "Before you begin, please make sure you have the necessary libraries installed. You can typically install them using pip.\n",
    "```bash\n",
    "pip install torch numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: GloVe Embeddings\n",
    "In this section, you will load the traditional GloVe embeddings, and explore the basic properties of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-13 20:39:33--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2025-11-13 20:39:34--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2025-11-13 20:39:35--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip.2’\n",
      "\n",
      "glove.6B.zip.2      100%[===================>] 822.24M  5.04MB/s    in 3m 44s  \n",
      "\n",
      "2025-11-13 20:43:20 (3.67 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "# Download the GloVe text files\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the GloVe embeddings [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe word vectors from a text file into a dictionary\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    \"\"\"\n",
    "    Loads GloVe embeddings into a dictionary mapping words to their vector representations.\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "    # TODO: Open the file and parse each line.\n",
    "    # Each line contains a word followed by its vector values.\n",
    "    # Convert the vector values to a numpy array.\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_dict[word] = vector\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_file_path = 'glove.6B.100d.txt'  # Replace with the path to your GloVe file\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Find closest words [5 pts]\n",
    "For a given word, you should find the most similar words in the vocabulary based on cosine similarity and output them along with their similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words closest to 'man' are:\n",
      "woman with similarity of 0.8323\n",
      "boy with similarity of 0.7915\n",
      "one with similarity of 0.7789\n",
      "person with similarity of 0.7527\n",
      "another with similarity of 0.7522\n"
     ]
    }
   ],
   "source": [
    "# Function to find the closest words and their corresponding similarity values\n",
    "def find_closest_words(word_vec, embeddings_dict, top_n=5):\n",
    "    \"\"\"\n",
    "    Finds the top_n closest words to a given vector based on cosine similarity.\n",
    "    Note: The input is a vector, not a word. This makes the function more versatile.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1. Calculate the cosine similarity between the input word_vec and all other word vectors in embeddings_dict.\n",
    "    # 2. Sort the words based on similarity in descending order.\n",
    "    # 3. Return the top_n words and their similarity scores.\n",
    "    keys = list(embeddings_dict.keys())\n",
    "    mat = np.array(list(embeddings_dict.values()))\n",
    "    # cosine similarity\n",
    "    mat_norms = np.linalg.norm(mat, axis=1)\n",
    "    vec_norm = np.linalg.norm(word_vec)\n",
    "    sims = (mat @ word_vec) / (mat_norms * vec_norm + 1e-9)\n",
    "    # sort by similarity descending and skip the identical vector (if present)\n",
    "    sorted_idx = np.argsort(-sims)\n",
    "    results = []\n",
    "    for idx in sorted_idx:\n",
    "        if np.allclose(mat[idx], word_vec, atol=1e-6):\n",
    "            continue\n",
    "        results.append((keys[idx], float(sims[idx])))\n",
    "        if len(results) >= top_n:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "chosen_word = 'man'\n",
    "if chosen_word in glove_embeddings:\n",
    "    chosen_word_vec = glove_embeddings[chosen_word]\n",
    "    closest_words = find_closest_words(chosen_word_vec, glove_embeddings, top_n=5)\n",
    "    print(f\"The words closest to '{chosen_word}' are:\")\n",
    "    for word, similarity in closest_words:\n",
    "        print(f\"{word} with similarity of {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Find new analogies [5 pts]\n",
    "In the lecture, we discussed how linear relationships exist in the embedding space (e.g. king - man + woman ≈ queen). Please demonstrate a new analogy that was not mentioned in the lecture. You should perform the vector arithmetic like `vec(word1) - vec(word2) + vec(word3)` and find the word closest to the resulting vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The words closest to the vector 'king - man + woman' are:\n",
      "king with similarity of 0.8552\n",
      "queen with similarity of 0.7834\n",
      "monarch with similarity of 0.6934\n",
      "throne with similarity of 0.6833\n",
      "daughter with similarity of 0.6809\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "vec = glove_embeddings['king'] - glove_embeddings['man'] + glove_embeddings['woman']\n",
    "closest_words = find_closest_words(vec, glove_embeddings, top_n=5)\n",
    "print(f\"\\nThe words closest to the vector 'king - man + woman' are:\")\n",
    "for word, similarity in closest_words:\n",
    "    print(f\"{word} with similarity of {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: BPE tokenizer\n",
    "\n",
    "Byte Pair Encoding(BPE) is a subword tokenization technique that iteratively merges the most frequent adjacent byte pairs into subword units, creating a vocabulary that balances character-level granularity and whole-word tokens. This method is widely used in modern natural language processing to handle out-of-vocabulary words and optimize tokenization efficiency.\n",
    "\n",
    "Let's look at an example. Given a sample string \"banana bandana\", we can calculate the frequency of the character pairs: \n",
    "```python\n",
    "('a', 'n'): 4, ('n', 'a'): 3, ('b', 'a'): 2, ('a', ' '): 1, (' ','b'): 1, ('n', 'd'): 1, ('d', 'a'): 1\n",
    "```\n",
    "Which means that we can combine 'an' into a new token. In the next round, 'an' can now participate in the frequency count, giving:\n",
    "```python\n",
    "('b', 'an'): 2, ('an', 'a'): 2, ('an', 'an'): 1, ('a', ' '): 1, (' ','b'): 1, ('an', 'd'): 1, ('d', 'an'): 1\n",
    "```\n",
    "So we may get 'ban' as a new token. Similarly, 'ana' would be the most frequent pair in the next round. With three merges, we've added 'an', 'ban' and 'ana' into our vocabulary, and our string can now be converted to the following tokens:\n",
    "```python\n",
    "'ban', 'ana', ' ', 'ban','d' ,'ana'\n",
    "```\n",
    "So now we can use 6 tokens to represent the 14 characters.\n",
    "\n",
    "You may wonder how this is better than word-level tokenization. First of all, it is more robust in out-of-vocabulary scenarios. For example, though the word \"bandana\" does exist in the GloVe embedding (look it up if you're not convinced), something like \"banada\" does not. When using GloVe embeddings, encountering \"banada\" during training would result in the default \\<UNK\\> token. In contrast, a BPE tokenizer can still infer the word's meaning through its sub-word tokens. Secondly, sub-word tokens include prefixes and suffixes that allow the model to learn different variations of a single word more efficiently.\n",
    "\n",
    "In this section, you are required to implement a BPE tokenizer, and use one of the provided corpora to train it. You may train it on character level (starting with a vocabulary of all characters in the corpus) or byte level (starting with a vocabulary of all 256 possible byte values). You should verify that encoding and then decoding a sentence produces the original sentence. You may refer to (but not copy) the following implementations:\n",
    "1. The tiktoken library: https://github.com/openai/tiktoken/blob/main/tiktoken/_educational.py\n",
    "2. Kaparthy's minbpe repository: https://github.com/karpathy/minbpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementation and Verification [15 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import collections\n",
    "class BPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "        self.next_id = 0\n",
    "        self.merges = {}\n",
    "    \n",
    "    def _get_stats(self, tokens: List[str]) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"计算相邻token对的频率\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pairs[tuple(tokens[i:i+2])] += 1\n",
    "        return pairs\n",
    "    def _merge_pair(self, tokens: List[str], pair: Tuple[str, str], new_token: str) -> List[str]:\n",
    "        \"\"\"将tokens列表中所有出现的pair替换为new_token\"\"\"\n",
    "        i = 0\n",
    "        new_tokens = []\n",
    "        while i < len(tokens):\n",
    "            # 检查当前位置和下一个位置是否形成要合并的pair\n",
    "            if i + 1 < len(tokens) and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                new_tokens.append(new_token)\n",
    "                i += 2  # 跳过已被合并的两个旧token\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return new_tokens\n",
    "    def train(self, text: str, vocab_size: int):\n",
    "        \"\"\"\n",
    "        Trains the BPE tokenizer. You will need to store the learned merge rules and the final vocabulary.\n",
    "        \"\"\"\n",
    "        # TODO:\n",
    "        # 1. Initialize vocabulary with all unique characters in the text.\n",
    "        # 2. Calculate the number of merges needed (vocab_size - initial_vocab_size).\n",
    "        # 3. Loop for the required number of merges:\n",
    "        #    a. Find the most frequent adjacent pair of tokens in the text.\n",
    "        #    b. Create a new token by merging this pair.\n",
    "        #    c. Add the new token to the vocabulary and record the merge rule.\n",
    "        #    d. Replace all occurrences of the pair in the text with the new token.\n",
    "\n",
    "        initial_chars = sorted(list(set(text)))\n",
    "        self.next_id = 0\n",
    "        for char in initial_chars:\n",
    "            self.vocab[char] = self.next_id\n",
    "            self.reverse_vocab[self.next_id] = char\n",
    "            self.next_id += 1\n",
    "            \n",
    "        initial_vocab_size = len(self.vocab)\n",
    "        num_merges = vocab_size - initial_vocab_size\n",
    "        tokens = list(text) \n",
    "        self.merges = []\n",
    "        for merge_i in range(num_merges):\n",
    "            # 2a. 统计最高频对\n",
    "            # 注意：这里直接作用于整个 tokens 列表\n",
    "            stats = self._get_stats(tokens)\n",
    "                    \n",
    "            if not stats:\n",
    "                break \n",
    "\n",
    "            best_pair = max(stats, key=stats.get)\n",
    "            \n",
    "            # 2b. 创建新 token\n",
    "            new_token_str = best_pair[0] + best_pair[1]\n",
    "            \n",
    "            # 2c. 添加新 token 到词汇表并记录规则（按顺序）\n",
    "            self.merges.append((best_pair, new_token_str))\n",
    "            self.vocab[new_token_str] = self.next_id\n",
    "            self.reverse_vocab[self.next_id] = new_token_str\n",
    "            self.next_id += 1\n",
    "            if merge_i < 5:\n",
    "                print(f\"Merge {merge_i+1}: '{best_pair[0]}', '{best_pair[1]}' -> '{new_token_str}'\")\n",
    "            \n",
    "            # 2d. 替换所有出现的pair\n",
    "            tokens = self._merge_pair(tokens, best_pair, new_token_str)\n",
    "        \n",
    "    def _tokenize_string(self, s: str) -> List[str]:\n",
    "        \"\"\"将单个字符串 s 转换为其最高效的BPE tokens表示\"\"\"\n",
    "        tokens = list(s)\n",
    "        \n",
    "        # 严格按照训练时记录的规则顺序进行合并\n",
    "        for pair, new_token_str in self.merges:\n",
    "            tokens = self._merge_pair(tokens, pair, new_token_str)\n",
    "            \n",
    "        return tokens\n",
    "            \n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encodes a string into a list of token indices using the learned merge rules.\n",
    "        \"\"\"\n",
    "        # 1. 应用所有合并规则\n",
    "        tokens = self._tokenize_string(text)\n",
    "        \n",
    "        # 2. 将最终 tokens 映射为 ID\n",
    "        encoded_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                encoded_tokens.append(self.vocab[token])\n",
    "            else:\n",
    "                # OOV 策略：对于未登录词，回退到其基础字符编码 (char-level OOV)\n",
    "                # 这在字符级BPE中是安全的，因为所有基础字符都存在于 vocab 中\n",
    "                for char in list(token):\n",
    "                    encoded_tokens.append(self.vocab[char])\n",
    "        \n",
    "        return encoded_tokens\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a list of token indices back into a text string.\n",
    "        \"\"\"\n",
    "        # 将 token ID 转换回 token 字符串\n",
    "        # 由于我们训练时没有丢弃或特殊处理空格，这里简单拼接即可精确重构\n",
    "        token_strings = [self.reverse_vocab[token_id] for token_id in tokens]\n",
    "        \n",
    "        return \"\".join(token_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1: 'e', ' ' -> 'e '\n",
      "Merge 2: 't', 'h' -> 'th'\n",
      "Merge 3: 't', ' ' -> 't '\n",
      "Merge 4: 's', ' ' -> 's '\n",
      "Merge 5: 'd', ' ' -> 'd '\n",
      "Verification Successful!\n",
      "Original string: O Romeo, Romeo, wherefore art thou Romeo?\n",
      "Encoded tokens: [27, 1, 30, 101, 43, 294, 30, 101, 43, 294, 191, 365, 464, 81, 67, 455, 30, 101, 43, 53, 12]\n",
      "Decoded string: O Romeo, Romeo, wherefore art thou Romeo?\n"
     ]
    }
   ],
   "source": [
    "# Load your training text here, we alse provide some sample text for you\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text = f.read()\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "# We will create a small vocabulary for demonstration purposes\n",
    "tokenizer.train(train_text, vocab_size=512)\n",
    "\n",
    "# Verification step\n",
    "test_string = \"O Romeo, Romeo, wherefore art thou Romeo?\"\n",
    "encoded_tokens = tokenizer.encode(test_string)\n",
    "decoded_string = tokenizer.decode(encoded_tokens)\n",
    "\n",
    "assert decoded_string == test_string\n",
    "print(\"Verification Successful!\")\n",
    "print(f\"Original string: {test_string}\")\n",
    "print(f\"Encoded tokens: {encoded_tokens}\")\n",
    "print(f\"Decoded string: {decoded_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Question [5 pts]\n",
    "\n",
    "**Question:** List the first 5 merge rules your tokenizer learned during training. Based on the `tinyshakespeare.txt` corpus, provide a brief explanation for why these specific pairs of characters were likely the first to be merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "*Your answer here. You should list the merges (e.g., ('t', 'h') -> 'th') and explain their high frequency in the context of Shakespearean English.*\n",
    "\n",
    "Merge 1: 'e', ' ' -> 'e '\n",
    "Merge 2: 't', 'h' -> 'th'\n",
    "Merge 3: 't', ' ' -> 't '\n",
    "Merge 4: 's', ' ' -> 's '\n",
    "Merge 5: 'd', ' ' -> 'd '\n",
    "\n",
    "Merge 1,3,4,5展示了单词末尾和空格的合并。这是很合理的。e,t,s,d都是常见的单词末尾字母。e是最常见的字母;art,not,it,but;his,lords,三单，复数等以s结尾；d构成过去式词尾（ed 的一部分）和高频词（如 and, had, lord）。  \n",
    "th是最经典的二元组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Text generation\n",
    "\n",
    "In this section, you will implement an LSTM-based model to generate sentences that mimic the style of the Shakespearean corpus. You will use the GloVe embeddings from Section 1 as a pre-trained embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages. Feel free to add ones that you need.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load and preprocess text [5 pts]\n",
    "You can choose a training corpus from the provided texts. Though the texts are much cleaner than random web crawls, you may still want to perform some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'([.,?!;:\"()])', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = text.split(' ')\n",
    "    return tokens\n",
    "\n",
    "processed_text = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build vocabulary and setup embedding matrix [5 pts]\n",
    "Create a vocabulary from your processed text. Then, create an embedding matrix where the i-th row corresponds to the GloVe vector for the i-th word in your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary from the processed text\n",
    "# TODO: Create a set of unique words, then create word-to-index (word2idx) and index-to-word (idx2word) mappings.\n",
    "\n",
    "\n",
    "# Create the embedding matrix from GloVe\n",
    "def create_embedding_matrix(word2idx, glove_embeddings, embedding_dim):\n",
    "    # Initialize matrix with zeros\n",
    "    embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
    "    # TODO: \n",
    "    # For each word in your vocabulary, if it exists in glove_embeddings, \n",
    "    # add its vector to the matrix at the correct index.\n",
    "    # Words not found in GloVe will remain as zero vectors.\n",
    "    for word, idx in word2idx.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "unique_words = sorted(list(set(processed_text)))\n",
    "word2idx = {word: i for i, word in enumerate(unique_words)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix(word2idx, glove_embeddings, embedding_dim)\n",
    "token_indices = [word2idx[word] for word in processed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Implement the dataset [10 pts]\n",
    "The text generation task uses next-word prediction as its objective. You should construct your dataset using a sliding window approach. For a sequence of length `n`, the first `n-1` words will be the input, and the `n`-th word will be the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct your dataset\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, token_indices, seq_length):\n",
    "        # TODO: Create input sequences and their corresponding targets.\n",
    "        # For a given seq_length, each sample should be (sequence_of_indices, next_word_index).\n",
    "        self.token_indices = token_indices\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        return len(self.token_indices) - self.seq_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO\n",
    "\n",
    "        window = self.token_indices[idx:idx + self.seq_length]\n",
    "\n",
    "        # Split into input and target\n",
    "        x = torch.tensor(window[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(window[-1], dtype=torch.long)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Implement the LSTM model [10 pts]\n",
    "You will use `nn.Embedding`, `nn.LSTM`, and `nn.Linear` to build your model. The embedding layer should be initialized with the pre-trained GloVe matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct your model\n",
    "class TextGenLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix):\n",
    "        super(TextGenLSTM, self).__init__()\n",
    "        # TODO:  \n",
    "        # 1. Create an embedding layer (nn.Embedding). Load the pre-trained embedding_matrix and set freeze=True to prevent it from being trained.\n",
    "        # 2. Create an LSTM layer (nn.LSTM).\n",
    "        # 3. Create a fully connected layer (nn.Linear) to map LSTM output to vocabulary size.\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # TODO\n",
    "        embed_out = self.embedding(x)\n",
    "        # lstm_out shape: (batch_size, seq_len, hidden_dim)\n",
    "        # hidden shape: ( (num_layers, batch_size, hidden_dim), ... )\n",
    "        lstm_out, hidden = self.lstm(embed_out, hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # logits shape: (batch_size, seq_len, vocab_size)\n",
    "        logits = self.fc(lstm_out)\n",
    "        logits = logits[:, -1, :]  # Get the logits for the last time step\n",
    "        return logits, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Implement a generate_text function [10 pts]\n",
    "This function will take a starting sequence (prompt) and generate a specified number of new words. To get more interesting results than simple greedy decoding (always picking the most probable word), try implementing a sampling strategy like top-k sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with your model\n",
    "def generate_text(model, start_sequence, num_words_to_generate, word2idx, idx2word, device, top_k=5):\n",
    "    \"\"\"\n",
    "    Generates text using the trained model and a top-k sampling strategy.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1. Set the model to evaluation mode.\n",
    "    # 2. Convert start_sequence to a tensor of indices.\n",
    "    # 3. Generate one word at a time for num_words_to_generate:\n",
    "    #    a. Feed the current sequence to the model.\n",
    "    #    b. Get the output logits for the next word.\n",
    "    #    c. Apply top-k sampling: get the top k logits and their indices, convert them to probabilities using softmax, and sample from this new distribution.\n",
    "    #    d. Append the sampled word's index to the sequence and use it as input for the next step.\n",
    "    # 4. Convert the final sequence of indices back to words and return as a string.\n",
    "    model.eval()\n",
    "    global SEQ_LENGTH \n",
    "    input_length = SEQ_LENGTH\n",
    "        \n",
    "    generated_indices = [word2idx[word] for word in start_sequence.split() if word in word2idx]\n",
    "    \n",
    "    # 确保起始序列长度不超过 input_length\n",
    "    current_indices = generated_indices[-input_length:]    \n",
    "    input_seq = torch.tensor(current_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "    \n",
    "    for _ in range(num_words_to_generate):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            \n",
    "            # 修正：output 已经是 (1, Vocab_size) 维度，移除 Batch 维\n",
    "            logits = output.squeeze(0) # 尺寸现在是 (Vocab_size)\n",
    "            \n",
    "            top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "            probabilities = torch.softmax(top_k_logits, dim=-1)\n",
    "            sampled_index_in_k = torch.multinomial(probabilities, 1).item()\n",
    "            next_word_index = top_k_indices[sampled_index_in_k].item()\n",
    "            \n",
    "            # 3. 更新序列\n",
    "            generated_indices.append(next_word_index)\n",
    "            current_indices = generated_indices[-input_length:]\n",
    "            input_seq = torch.tensor(current_indices, dtype=torch.long).unsqueeze(0).to(device)    \n",
    "            \n",
    "    generated_words = [idx2word.get(idx, '<UNK>') for idx in generated_indices]\n",
    "    return \" \".join(generated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Implement the training loop [10 pts]\n",
    "Train your model. During each epoch, log the average training and validation loss. It's also highly recommended to generate a short piece of text after each epoch to see how the model's creative abilities evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # TODO: Your training steps here (zero grad, forward pass, loss, backward, step)\n",
    "            # Remember to detach the hidden state to prevent backpropagating through the entire history.\n",
    "            optimizer.zero_grad()\n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                hidden = model.init_hidden(inputs.size(0)) \n",
    "                outputs, _ = model(inputs, hidden)                 \n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # Logging and generating sample text\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # TODO: Call your generate_text function with a fixed prompt (e.g., \"shall i compare thee\")\n",
    "        # and print the generated text to observe the model's progress.\n",
    "        prompt = \"shall i compare thee\"\n",
    "        generated_text = generate_text(model, prompt, num_words_to_generate=20, word2idx=word2idx, idx2word=idx2word, device=device, top_k=5)\n",
    "        print(f'Generated Text: {generated_text}\\n')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda...\n",
      "Epoch 1/10, Train Loss: 6.0144, Val Loss: 5.7731\n",
      "Generated Text: shall i compare thee , i am a of a : and he have , that is the world , and the his ,\n",
      "\n",
      "Epoch 2/10, Train Loss: 5.5215, Val Loss: 5.5843\n",
      "Generated Text: shall i compare thee , and my lord . king henry vi : the queen is a man . gloucester : my lord ,\n",
      "\n",
      "Epoch 3/10, Train Loss: 5.3270, Val Loss: 5.4900\n",
      "Generated Text: shall i compare thee ; and i have , my father . king henry vi richard : i have not to be , and\n",
      "\n",
      "Epoch 4/10, Train Loss: 5.1919, Val Loss: 5.4429\n",
      "Generated Text: shall i compare thee , and i have , for that i will be a little man . romeo : o , thou art\n",
      "\n",
      "Epoch 5/10, Train Loss: 5.0902, Val Loss: 5.4056\n",
      "Generated Text: shall i compare thee , to thy love to the tower , and the duke of york . king edward iv : i know\n",
      "\n",
      "Epoch 6/10, Train Loss: 5.0081, Val Loss: 5.3851\n",
      "Generated Text: shall i compare thee , and i will not so , that i have been a man of my heart , and i will\n",
      "\n",
      "Epoch 7/10, Train Loss: 4.9389, Val Loss: 5.3713\n",
      "Generated Text: shall i compare thee , for thou shalt not so , i am not not . duke vincentio : i am a man of\n",
      "\n",
      "Epoch 8/10, Train Loss: 4.8808, Val Loss: 5.3589\n",
      "Generated Text: shall i compare thee you . romeo : o , i am a word . i have a man to be , that thou\n",
      "\n",
      "Epoch 9/10, Train Loss: 4.8295, Val Loss: 5.3548\n",
      "Generated Text: shall i compare thee , and let me to the crown , to make his oath , and the king . king richard ii\n",
      "\n",
      "Epoch 10/10, Train Loss: 4.7850, Val Loss: 5.3529\n",
      "Generated Text: shall i compare thee to my soul . i was not so : i am a little man . nurse , my lord ,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize hyperparameters, model, optimizer, etc., and start the training process.\n",
    "# TODO\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# 3. 数据集和 DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "split_idx = int(len(token_indices) * 0.9)  # 保持 90% 训练集 (匹配你的 test_size=0.1)\n",
    "train_indices = token_indices[:split_idx]\n",
    "val_indices = token_indices[split_idx:]\n",
    "\n",
    "# --- (以下代码保持不变) ---\n",
    "# 创建 Dataset\n",
    "train_dataset = TextDataset(train_indices, SEQ_LENGTH)\n",
    "val_dataset = TextDataset(val_indices, SEQ_LENGTH)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 4. 初始化模型、损失函数和优化器\n",
    "model = TextGenLSTM(vocab_size, embedding_dim, HIDDEN_DIM, NUM_LAYERS, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,weight_decay=1e-5)\n",
    "\n",
    "# 5. 启动训练\n",
    "print(f\"Starting training on {device}...\")\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, device, \n",
    "                            epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Question: Design Choices [15 pts]\n",
    "\n",
    "**Question:** Discuss **at least two** design choices you made during the implementation of your text generation model (Section 3) and explain how they impacted the final result. You can discuss any of the steps, from text preprocessing and dataset construction to model architecture and the text generation strategy.\n",
    "\n",
    "For each choice, describe:\n",
    "1.  **What was the choice?** (e.g., sequence length in the dataset, number of LSTM layers, using top-k sampling vs. greedy decoding).\n",
    "2.  **What was your rationale for this choice?** (e.g., 'I chose a longer sequence length to capture more context...' or 'I used top-k sampling to avoid repetitive text...').\n",
    "3.  **How did it affect the outcome?** (e.g., 'This resulted in more coherent sentences but increased training time.' or 'The generated text became more diverse and less predictable.').\n",
    "\n",
    "*Your answer will be evaluated based on the clarity and depth of your rationale. **Please note:** The goal of this question is to encourage reflection. As long as you clearly explain your choices and your reasoning, you will receive full credit, so you don't need to write a lot and worry about losing points.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda...\n",
      "Epoch 1/10, Train Loss: 6.2935, Val Loss: 6.1014\n",
      "Generated Text: shall i compare thee . and and the the the a the the , and and and i , the the the his ,\n",
      "\n",
      "Epoch 2/10, Train Loss: 6.0136, Val Loss: 5.9756\n",
      "Generated Text: shall i compare thee to my of of his of of my lord , and you you have , the the is , that\n",
      "\n",
      "Epoch 3/10, Train Loss: 5.9379, Val Loss: 5.9700\n",
      "Generated Text: shall i compare thee have you to , that the this king . king ! , that he have i , that , the\n",
      "\n",
      "Epoch 4/10, Train Loss: 5.9098, Val Loss: 5.9249\n",
      "Generated Text: shall i compare thee . duke , and you have , i have be the the , and the the lord . the york\n",
      "\n",
      "Epoch 5/10, Train Loss: 5.8967, Val Loss: 5.9248\n",
      "Generated Text: shall i compare thee , but is , i have you to him to to , i have not , to me . and\n",
      "\n",
      "Epoch 6/10, Train Loss: 5.8881, Val Loss: 5.9281\n",
      "Generated Text: shall i compare thee . the york . the the king , but you not the the lord ; and the the the is\n",
      "\n",
      "Epoch 7/10, Train Loss: 5.8836, Val Loss: 5.9280\n",
      "Generated Text: shall i compare thee , but you , and my , and the the the the of the lord . i have be be\n",
      "\n",
      "Epoch 8/10, Train Loss: 5.8820, Val Loss: 5.9062\n",
      "Generated Text: shall i compare thee , but , that you i , the , and that , and my , and that , the a\n",
      "\n",
      "Epoch 9/10, Train Loss: 5.8774, Val Loss: 5.9246\n",
      "Generated Text: shall i compare thee , i , my will be to the the king . king : what , i am , the my\n",
      "\n",
      "Epoch 10/10, Train Loss: 5.8762, Val Loss: 5.8996\n",
      "Generated Text: shall i compare thee i have not , and the the king , i will be be to the a . , and you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize hyperparameters, model, optimizer, etc., and start the training process.\n",
    "# TODO\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SEQ_LENGTH = 30\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# 3. 数据集和 DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "split_idx = int(len(token_indices) * 0.9)  # 保持 90% 训练集 (匹配你的 test_size=0.1)\n",
    "train_indices = token_indices[:split_idx]\n",
    "val_indices = token_indices[split_idx:]\n",
    "\n",
    "# --- (以下代码保持不变) ---\n",
    "# 创建 Dataset\n",
    "train_dataset = TextDataset(train_indices, SEQ_LENGTH)\n",
    "val_dataset = TextDataset(val_indices, SEQ_LENGTH)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 4. 初始化模型、损失函数和优化器\n",
    "model = TextGenLSTM(vocab_size, embedding_dim, HIDDEN_DIM, NUM_LAYERS, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,weight_decay=1e-3)\n",
    " \n",
    "# 5. 启动训练\n",
    "print(f\"Starting training on {device}...\")\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, device, \n",
    "                            epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda...\n",
      "Epoch 1/10, Train Loss: 6.0656, Val Loss: 5.8638\n",
      "Generated Text: shall i compare thee . first : you have not be be , but i have , to you to you to the ,\n",
      "\n",
      "Epoch 2/10, Train Loss: 5.5084, Val Loss: 5.7051\n",
      "Generated Text: shall i compare thee , i am the world , and you have to the king ? what is my father ? o ,\n",
      "\n",
      "Epoch 3/10, Train Loss: 5.2839, Val Loss: 5.6702\n",
      "Generated Text: shall i compare thee to my brother ; and you , and i have been to the king ; and that you will not\n",
      "\n",
      "Epoch 4/10, Train Loss: 5.1194, Val Loss: 5.6639\n",
      "Generated Text: shall i compare thee : and , i do , and you have heard to the world , that we shall be a thousand\n",
      "\n",
      "Epoch 5/10, Train Loss: 4.9792, Val Loss: 5.6878\n",
      "Generated Text: shall i compare thee , and , i am not , i will not . i will not . king richard iii : i\n",
      "\n",
      "Epoch 6/10, Train Loss: 4.8584, Val Loss: 5.7063\n",
      "Generated Text: shall i compare thee , and , for i would not have been a happy . king henry vi : why , i am\n",
      "\n",
      "Epoch 7/10, Train Loss: 4.7489, Val Loss: 5.7787\n",
      "Generated Text: shall i compare thee , and so i am a king , i have done the duke of york . queen elizabeth : i\n",
      "\n",
      "Epoch 8/10, Train Loss: 4.6491, Val Loss: 5.8385\n",
      "Generated Text: shall i compare thee ; and thou wilt not not , i am sorry , for the duke of buckingham ? gloucester : i\n",
      "\n",
      "Epoch 9/10, Train Loss: 4.5596, Val Loss: 5.8903\n",
      "Generated Text: shall i compare thee . queen margaret : i do not , for the world : what is my father ? gloucester : i\n",
      "\n",
      "Epoch 10/10, Train Loss: 4.4766, Val Loss: 5.9517\n",
      "Generated Text: shall i compare thee , but thou wilt be not ; i will not , i am a king of york : and that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize hyperparameters, model, optimizer, etc., and start the training process.\n",
    "# TODO\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# 3. 数据集和 DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "split_idx = int(len(token_indices) * 0.9)  # 保持 90% 训练集 (匹配你的 test_size=0.1)\n",
    "train_indices = token_indices[:split_idx]\n",
    "val_indices = token_indices[split_idx:]\n",
    "\n",
    "# --- (以下代码保持不变) ---\n",
    "# 创建 Dataset\n",
    "train_dataset = TextDataset(train_indices, SEQ_LENGTH)\n",
    "val_dataset = TextDataset(val_indices, SEQ_LENGTH)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 4. 初始化模型、损失函数和优化器\n",
    "model = TextGenLSTM(vocab_size, embedding_dim, HIDDEN_DIM, NUM_LAYERS, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 5. 启动训练\n",
    "print(f\"Starting training on {device}...\")\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, device, \n",
    "                            epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda...\n",
      "Epoch 1/10, Train Loss: 6.3057, Val Loss: 6.1139\n",
      "Generated Text: shall i compare thee . i i , , i , , the my , and the the the . i be , that\n",
      "\n",
      "Epoch 2/10, Train Loss: 6.0301, Val Loss: 6.0097\n",
      "Generated Text: shall i compare thee . the , and and my lord , and my lord , and i , and , that have have\n",
      "\n",
      "Epoch 3/10, Train Loss: 5.9571, Val Loss: 5.9806\n",
      "Generated Text: shall i compare thee i will the my , and my , to the a the king : and i have be you have\n",
      "\n",
      "Epoch 4/10, Train Loss: 5.9269, Val Loss: 5.9652\n",
      "Generated Text: shall i compare thee have be be the , and , my lord , and i am , i have i do have ,\n",
      "\n",
      "Epoch 5/10, Train Loss: 5.9140, Val Loss: 5.9458\n",
      "Generated Text: shall i compare thee , and i have be not be to the my lord . king of the lord , my , and\n",
      "\n",
      "Epoch 6/10, Train Loss: 5.9068, Val Loss: 5.9354\n",
      "Generated Text: shall i compare thee i , the a , and and the , and my lord , and you have the a of ,\n",
      "\n",
      "Epoch 7/10, Train Loss: 5.9019, Val Loss: 5.9478\n",
      "Generated Text: shall i compare thee have to , and the the , and that the a the of a of a , to be be\n",
      "\n",
      "Epoch 8/10, Train Loss: 5.8996, Val Loss: 5.9403\n",
      "Generated Text: shall i compare thee , and i have i , the lord . and i , the , to be , and , i\n",
      "\n",
      "Epoch 9/10, Train Loss: 5.8967, Val Loss: 5.9403\n",
      "Generated Text: shall i compare thee you have not you to the the . and i am , the the , and you you , i\n",
      "\n",
      "Epoch 10/10, Train Loss: 5.8940, Val Loss: 5.9352\n",
      "Generated Text: shall i compare thee will the the . i would the the the , that i will not the the . king richard :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize hyperparameters, model, optimizer, etc., and start the training process.\n",
    "# TODO\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# 3. 数据集和 DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "split_idx = int(len(token_indices) * 0.9)  # 保持 90% 训练集 (匹配你的 test_size=0.1)\n",
    "train_indices = token_indices[:split_idx]\n",
    "val_indices = token_indices[split_idx:]\n",
    "\n",
    "# --- (以下代码保持不变) ---\n",
    "# 创建 Dataset\n",
    "train_dataset = TextDataset(train_indices, SEQ_LENGTH)\n",
    "val_dataset = TextDataset(val_indices, SEQ_LENGTH)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 4. 初始化模型、损失函数和优化器\n",
    "model = TextGenLSTM(vocab_size, embedding_dim, HIDDEN_DIM, NUM_LAYERS, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,weight_decay=1e-3)\n",
    "\n",
    "# 5. 启动训练\n",
    "print(f\"Starting training on {device}...\")\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, device, \n",
    "                            epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee , but that i am not , and , i am not , for i will not , but ,\n",
      "shall i compare thee now , but i am not to hear it , and so my lord . lady anne : ay ,\n",
      "shall i compare thee , and i am a king , and i am not to be revenged . king richard ii : i\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(trained_model, \"shall i compare thee\", num_words_to_generate=20, word2idx=word2idx, idx2word=idx2word, device=device, top_k=5))\n",
    "print(generate_text(trained_model, \"shall i compare thee\", num_words_to_generate=20, word2idx=word2idx, idx2word=idx2word, device=device, top_k=10))\n",
    "print(generate_text(trained_model, \"shall i compare thee\", num_words_to_generate=20, word2idx=word2idx, idx2word=idx2word, device=device, top_k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "#### **选择 1：数据集序列长度**\n",
    "\n",
    "* **1. 选择了什么？**\n",
    "    我对比了两种不同的序列长度（`SEQ_LENGTH`）设置：30 和 60。\n",
    "* **2. 选择的理由是什么？**\n",
    "    旨在探索序列长度对训练效率（LSTM 的 O(n) 复杂度）和模型性能（更长的上下文 vs. 更短的上下文）之间的权衡。\n",
    "* **3. 这对结果有何影响？**\n",
    "    * **训练时间**：`SEQ_LENGTH=30` 的训练时间确实接近 `SEQ_LENGTH=60` 的一半，这与 LSTM 训练复杂度的理论预期一致。\n",
    "    * **模型损失**：`SEQ_LENGTH=30` 的模型最终收敛的 loss 偏高，这也符合预期，因为较短的序列提供了较少的上下文信息。\n",
    "    * **生成质量**：尽管 loss 有差异，但两种设置在*生成质量*上差异不大。这可能是因为用于测试的 prompt（提示）本身很短，远未达到 30 个词元的限制，因此模型无法利用更长的上下文优势。\n",
    "\n",
    "#### **选择 2：分词策略**\n",
    "\n",
    "* **1. 选择了什么？**\n",
    "    我没有使用简单的空格分割，而是实现了一个预处理步骤，通过在标点符号周围添加空格来将它们与单词显式分开（例如，`\"revenged.\"` 变为 `\"revenged\" \" . \"`)。\n",
    "* **2. 选择的理由是什么？**\n",
    "    预训练的 GloVe 词嵌入 是为“干净”的单词（不带标点符号）提供向量的。简单的分割会产生许多词汇表外 (OOV) 的词元，例如 `\"revenged.\"`，这些词元在词嵌入矩阵中是找不到的。通过将它们分开，我确保了有效的单词（`\"revenged\"`）能够正确映射到其预训练向量，从而最大限度地利用 GloVe 词嵌入。\n",
    "* **3. 这对结果有何影响？**\n",
    "    这个选择显著减少了 OOV 词元的数量。它使模型能够学习标点符号作为独立词元的句法作用，并改善了单词到其有意义嵌入的整体映射，从而提升了模型性能及其生成语法合理文本的能力。\n",
    "\n",
    "#### **选择 3：文本生成的采样策略**\n",
    "\n",
    "* **1. 选择了什么？**\n",
    "    我比较了三种不同的文本生成解码策略：贪心解码（Greedy Decoding，总是选择概率最高的词）、Top-k 采样（$k=5$）以及 Top-k 采样（$k=10$）。\n",
    "* **2. 选择的理由是什么？**\n",
    "    目标是在文本的流畅性 (fluency) 和多样性 (diversity) 之间找到平衡。贪心解码通常会导致重复的、确定性的、乏味的文本。引入采样（如 Top-k）可以增加随机性，从而创造出更多样化、“更有创意”的文本，但过多的随机性（如过高的 'k' 值）可能导致文本 incoherent（不连贯）。\n",
    "* **3. 这对结果有何影响？**\n",
    "    结果显示了清晰的权衡。贪心解码产生的文本具有很高的局部流畅性，但很快变得重复，且词汇多样性很低。Top-10 采样产生的文本则经常在语法上不通顺，缺乏连贯性。Top-5 采样 提供了最佳的折衷：它生成的文本比贪心方法多样化得多，但又不像 Top-10 方法那样混乱，仍然保持了合理的句子结构和流畅度。\n",
    "\n",
    "#### **选择 4：正则化策略**\n",
    "\n",
    "* **1. 选择了什么？**\n",
    "    我试验了优化器中不同的 `weight_decay` (权重衰减) 值，特别是比较了低值 (1e-5)、中等值 (1e-3) 和无权重衰减这三种设置。\n",
    "* **2. 选择的理由是什么？**\n",
    "    `tinyshakespeare` 是一个极小的数据集，这使得模型非常容易发生过拟合。\n",
    "* **3. 这对结果有何影响？**\n",
    "    鉴于数据集很小，所有设置都显示出过拟合的迹象。然而，1e-5 的设置取得了最好的平衡。与没有衰减相比，它明显减缓了过拟合的发生速度；同时，它也比更激进的 1e-3 设置收敛到了一个更好（更低）的最终验证损失。这表明，在这个特定数据集上，1e-5 是寻找泛化解的最有效选择。\n",
    "\n",
    "注：为了更清晰，使用了AI帮助整理书写。原始文段：\n",
    "\n",
    " 对于序列长度，我选择了30和60两组。从训练时间上来看，序列长度为30的确实接近60的一半，这和LSTM训练时O(n)复杂度是一致的。从loss上看，序列长度为30训练的模型最终loss偏大，这符合我们对语句越长、信息越丰富的感知。但是整体上看，二者在生成的质量上差异不大。我认为这是合理的，因为生成的prompt序列长度很短，远远没有超过训练的30 token的限制。\n",
    "\n",
    "\n",
    "对于防止过拟合的weight decay，我实验了1e-5,1e-3，无三种settings。很明显，对于生成任务，这个数据集过于小了。无论是哪种settings都无法生成让人满意的语句，且都出现了一定的过拟合的情况。整体上来看，选择1e-5是一个比较平衡的举动，不仅过拟合现象相对缓解，收敛到的解也性质比较好，可以获得相对较低的loss.\n",
    "\n",
    "\n",
    "对于tokenizer,我选择了一种将标点符号单独分割出来的方法。由于原始的word embedding是相对干净的，通过将标点符号前后加空格强行分开的方法，避免了如 `revenged.` 这种非法token的出现，提升了模型的性能。\n",
    "\n",
    "\n",
    "关于采样策略，我分别尝试了top 5,top 10, greedy三种。greedy的结果虽然流畅度相对较高，但是生成词汇的多样性很差，而top10的语句通顺性相对较差，没有明显的实际意义。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
