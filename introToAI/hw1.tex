% vim:ft=tex:
%
\documentclass[UTF8]{ctexart}
\usepackage{amsmath}
\usepackage{listings}

\lstset{
    basicstyle          =   \sffamily,          % 基本代码风格
    keywordstyle        =   \bfseries,          % 关键字风格
    commentstyle        =   \rmfamily\itshape,  % 注释的风格，斜体
    stringstyle         =   \ttfamily,  % 字符串风格
    flexiblecolumns,                % 别问为什么，加上这个
    numbers             =   left,   % 行号的位置在左边
    showspaces          =   false,  % 是否显示空格，显示了有点乱，所以不现实了
    numberstyle         =   \zihao{-5}\ttfamily,    % 行号的样式，小五号，tt等宽字体
    showstringspaces    =   false,
    captionpos          =   t,      % 这段代码的名字所呈现的位置，t指的是top上面
    frame               =   lrtb,   % 显示边框
}

\lstdefinestyle{Python}{
    language        =   Python, % 语言选Python
    basicstyle      =   \zihao{-5}\ttfamily,
    numberstyle     =   \zihao{-5}\ttfamily,
    keywordstyle    =   \color{blue},
    keywordstyle    =   [2] \color{teal},
    stringstyle     =   \color{magenta},
    commentstyle    =   \color{red}\ttfamily,
    breaklines      =   true,   % 自动换行，建议不要写太长的行
    columns         =   fixed,  % 如果不加这一句，字间距就不固定，很丑，必须加
    basewidth       =   0.5em,
}
\title{
	人工智能基础作业1
}
\author{
	林乐天 --- \texttt{2300012154@stu.pku.edu.cn}
}

\begin{document}
\maketitle
\section{问答1}
\subsection{问题}请简述什么是贝叶斯定理，什么是最大似然估计(MLE)，什么是最大后验估计
(MAP)。
\subsection{答案}

\paragraph{贝叶斯定理}贝叶斯定理是一个描述对事件发生可能性的信心的公式，基于对已有可能与事件相关的先验知识来描述时间发生的可能性。其公式为：$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$，称$P(B|A)$先验概率，称$P(A|B)$为后验概率。

\paragraph{最大似然估计}最大似然估计是一种优化方法，是在假定数据随机无偏的情况下，通过使得$P(B|A)$最大的方法，使得模型最大限度的拟合现实情况，对数据集的要求很高。

\paragraph{最大后验估计}最大后验估计也是一种优化方法，本质上是不信任数据集能真实反映现实情况，通过假设一个先验分布，最大化$P(A|B)$即最大化$P(B|A)*P(A)$来优化模型的方法，对先验分布比较敏感。

\section{问答2}
\subsection{问题}
设$X~N(\mu, \sigma^2)$, $\mu,\sigma^2$为未知参数，$x_1\dots x_n$是来自$X$的样本值，求$\mu,\sigma^2$的最大似然估计量。
\subsection{答案}$$\mu = \frac{1}{n}\sum_i={1}^n x_n$$
$$\sigma^2 = \frac{1}{n}\sum_{i=1}^n(x_n - \mu)^2$$

\section{问答3}
\subsection{问题}
请简述分类问题与回归问题的主要区别。
\subsection{答案}
回归问题输出的是一个连续值，是对一个连续函数的预测和拟合，其标签是连续的;而分类问题输出的是一个离散值，是对一个离散值的预测，其标签是离散的。
\section{问答4}
\subsection{问题}
请简述有监督学习与无监督学习的主要区别。
\subsection{答案}
有监督学习除了给出数据，还给出了lable;无监督学习只给出数据，不给出label。

\section{问答5}
\subsection{问题}给定数据$D = /{(x_1, y_1), (x_2, y_2), … , (x_n, y_n)/}$，
用一个线性模型估计最接近真实$y$(ground truth )的连续标量 $Y$，$f(x_i ) = Wx_i + b$, such that $f(x) \approx y$.
求最优 $(w^*, b^*)$使得$f(x)$与$y$之间的均方误差最小
并解释$(w^*, b^*)$何时有 closed form 解，何时没有 closed form 解。
\subsection{答案}
先写出均方误差：$$L(W,b) = \frac{1}{n}\sum{Wx_i+b-y_i}$$

$$L(\beta)=\frac{1}{n}(A\beta-Y)^T(A\beta-Y)$$其中，$A$包含了数据的信息，且加了一行1;$\beta$是前文$W$和$b$的组合。

对$\beta$求偏导并求零点，化简得：$(A^TA)\beta = A^TY$

如果$(A^TA)$可逆，则其有closed form的解，否则没有。


\section{问答6}
\subsection{问题}Ridge regression 问题的解具有什么特点，为什么？Lasso 问题的解具有什么特点？为什
么？
\subsection{答案}
\paragraph{Ridge regression}
这种方法的解相对来说其绝对值较小。由于其为$L_2$距离，对绝对值大小比较敏感，且容易在非坐标轴的位置和解域相交，所以绝对值小。
\paragraph{Lasso regression}
这种方法的解相对维度较小，会出现大量的0。由于其为$L_1$距离，解域很容易在坐标轴上相交，于是如此。
\section{问答7}
\subsection{问题}
请从 model function、loss function、optimization solution 三个方面比较 Linear regression
与 Logistic regression 的异同。
\subsection{答案}
\paragraph{model function}
对于Linear来说，model function只是一个单纯的对X的linear conbination，在加上一个bias,是一个纯粹的线性形式$y=Wx+b$

而对于Logistic regression来说，为了让结果可以解释，要在线性组合外面套一个sigmoid函数，就使得整个函数并非完全的线性，输出值也被控制在了[0,1]上。

\paragraph{loss function}
对于Linear来说，更常用的是基本的Square Loss，由于取值范围相对较大，使用均方误差收敛速度尚可，故如此。

而对于Logistic来说，若使用Square Loss，会在结果严重偏离正确结果的时候不能收敛，其梯度会变成0，于是常用Cross Ertnopy Loss，从而保证其在梯度下降的时候可以正常运作。

\paragraph{optimization solution}
二者在这个方面具有一致性。两个函数都是用梯度下降的方法来进行优化的，所以二者在这个方面是相同的。


\section{问答8}
\subsection{问题}
K-近邻分类器的超参数是什么？怎么选择 K-近邻分类器的超参数？
\subsection{答案}
\paragraph{超参数}其超参数有两个，一个是K的值，另一个是距离量度的选择。
\paragraph{选择}通过在测试集上进行调试找到合适的超参数，在数据集不够大的时候可以进行交叉验证。
\end{document}
