import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from statsmodels.stats.outliers_influence import variance_inflation_factor


from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split 
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA


from sklearn.linear_model import LinearRegression 
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import ElasticNet

from sklearn import metrics
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import cross_val_score

from sklearn.feature_selection import VarianceThreshold, RFE, SelectKBest, f_regression

from scipy import stats


train = pd.read_csv("./data/train.csv")
train_y = train[["SALE PRICE"]]
train_X = train

test_X = pd.read_csv("./data/test.csv")
test_y = pd.read_csv("./data/test_groundtruth.csv")

print("train_X:",train_X.shape)
print("train_y:",train_y.shape)
print("test_X:",test_X.shape)
print("test_y:",test_y.shape)


train.head()


num_train_samples = len(train_X)

data_X = pd.concat([train_X, test_X])





print(train["LAND SQUARE FEET"].value_counts().sort_values(ascending=False).head(10))
train["LAND SQUARE FEET"].replace(" -  ","0",inplace=True)
train["LAND SQUARE FEET"] = train["LAND SQUARE FEET"].astype("int64")
print(train["LAND SQUARE FEET"].value_counts().sort_values(ascending=False).head(10))
sns.regplot(x="LAND SQUARE FEET",y="SALE PRICE",data=train[train["LAND SQUARE FEET"]!=0])
med=train[train["LAND SQUARE FEET"]!=0]["LAND SQUARE FEET"].median()

data_X["LAND SQUARE FEET"] = data_X["LAND SQUARE FEET"].replace(" -  ","0")
data_X["LAND SQUARE FEET"] = data_X["LAND SQUARE FEET"].astype("int64")
data_X['LAND SQUARE FEET'] = data_X["LAND SQUARE FEET"].replace(0,med)


print(train["GROSS SQUARE FEET"].value_counts().sort_values(ascending=False).head(10))
train["GROSS SQUARE FEET"].replace(" -  ","0",inplace=True)
train["GROSS SQUARE FEET"] = train["GROSS SQUARE FEET"].astype("int64")
print(train["GROSS SQUARE FEET"].value_counts().sort_values(ascending=False).head(10))
sns.regplot(x="GROSS SQUARE FEET",y="SALE PRICE",data=train[train["GROSS SQUARE FEET"]!=0])
med=train[train["GROSS SQUARE FEET"]!=0]["GROSS SQUARE FEET"].median() #2500

data_X["GROSS SQUARE FEET"] = data_X["GROSS SQUARE FEET"].replace(" -  ","0")
data_X["GROSS SQUARE FEET"] = data_X["GROSS SQUARE FEET"].astype("int64")
data_X["GROSS SQUARE FEET"] = data_X["GROSS SQUARE FEET"].replace(0,med) #用中位数填充0 最后证明并没有效果








del train_X['SALE PRICE']
del data_X['SALE PRICE']
#转化元素类型为catagory
data_X['TAX CLASS AT TIME OF SALE'] = data_X['TAX CLASS AT TIME OF SALE'].astype('category')
data_X['TAX CLASS AT PRESENT'] = data_X['TAX CLASS AT PRESENT'].astype('category')
data_X['BOROUGH'] = data_X['BOROUGH'].astype('category')
data_X['BUILDING CLASS CATEGORY'] = data_X['BUILDING CLASS CATEGORY'].astype('category')
data_X['BUILDING CLASS AT PRESENT'] = data_X['BUILDING CLASS AT PRESENT'].astype('category')
data_X['BUILDING CLASS AT TIME OF SALE'] = data_X['BUILDING CLASS AT TIME OF SALE'].astype('category')
data_X['date'] = pd.to_datetime(data_X['SALE DATE'])
# 提取时间特征
data_X['year'] = data_X['date'].dt.year
data_X['month'] = data_X['date'].dt.month
data_X['day'] = data_X['date'].dt.day


data_X.info()


del data_X['SALE DATE']
del data_X['date']
all_colm = data_X.columns
one_hot_features = ['BOROUGH','ADDRESS','APARTMENT NUMBER', 'BUILDING CLASS CATEGORY','TAX CLASS AT PRESENT','TAX CLASS AT TIME OF SALE','NEIGHBORHOOD','BUILDING CLASS AT PRESENT','ZIP CODE','BLOCK','BUILDING CLASS AT TIME OF SALE']


data_X.info()


data_X.head()


le = LabelEncoder()
for i in one_hot_features:
    data_X[i + '_encoded'] = le.fit_transform(data_X[i])


data_X.info()


numerical_columns = []
for i in all_colm:
    if i not in one_hot_features:
        numerical_columns.append(i)
# 提取数值列并进行标准化
scaler = StandardScaler()
standardized_numerical_data = scaler.fit_transform(data_X[numerical_columns])

# 创建一个新的 DataFrame 只包含标准化后的数值列
standardized_numerical_df = pd.DataFrame(standardized_numerical_data, columns=numerical_columns)
encoded_columns = [col + '_encoded' for col in one_hot_features]

# 确保索引唯一性并对齐
standardized_numerical_df.reset_index(drop=True, inplace=True)
data_X.reset_index(drop=True, inplace=True)

# 整合编码后的类别变量和标准化后的数值变量
standardized_df = pd.concat([data_X[encoded_columns], standardized_numerical_df], axis=1)
standardized_df


# 计算 VIF
vif_data = pd.DataFrame()
vif_data["feature"] = standardized_df.columns
vif_data["VIF"] = [variance_inflation_factor(standardized_df.values, i) for i in range(standardized_df.shape[1])]

print("标准化后的 VIF 数据:\n", vif_data)


#for i in encoded_columns:
#    standardized_df[i] = standardized_df[i].astype('category')

#one_hot_encoded = pd.get_dummies(standardized_df[encoded_columns])

#one_hot_encoded.info(verbose=True, memory_usage=True)


#PCA
# 定义 PCA 并拟合
pca = PCA(n_components=20)  # 选择适当的组件数量
X_pca = pca.fit_transform(standardized_df)


train_X = standardized_df[:num_train_samples]
test_X = standardized_df[num_train_samples:]





# 1. 初步特征选择（过滤法）
selector_var = VarianceThreshold(threshold=0.1)
X_train_var = selector_var.fit_transform(train_X)
X_test_var = selector_var.transform(test_X)

# 2. 特征重要性分析（嵌入法）
rf = RandomForestRegressor()
rf.fit(X_train_var, train_y)

importances = rf.feature_importances_
indices = importances.argsort()[-10:][::-1]
X_train_important = X_train_var[:, indices]
X_test_important = X_test_var[:, indices]

# 3. 高级特征选择（包装法）
estimator = RandomForestRegressor()
selector_rfe = RFE(estimator, n_features_to_select=5, step=1)
selector_rfe.fit(X_train_important, train_y)

X_train_final = selector_rfe.transform(X_train_important)
X_test_final = selector_rfe.transform(X_test_important)

# 4. 超参数调优
param_grid = {
    'n_estimators': [100, 200,300,400],
    'max_depth': [10, 20,30,40],
    'min_samples_split': [2, 5,10,15],

    'min_samples_leaf': [1, 2,3,4]
}


grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_final, train_y)

best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(X_test_final)


# 5. 评估
y_pred = best_rf.predict(X_test_final)
mse = mean_absolute_percentage_error(y_pred, test_y)

print(f"{mse}")
print(f"Best parameters:{best_rf} ")
pd.DataFrame({"pred":y_pred}).to_csv("学号_姓名.csv")


rf_regr = RandomForestRegressor()
rf_regr.fit(X_train_final, train_y)
Y_pred_training_rf = rf_regr.predict(X_train_final)
Y_pred_rf = rf_regr.predict(X_test_final)

# MAPE metric
print(mean_absolute_percentage_error(train_y,Y_pred_training_rf))
print(mean_absolute_percentage_error(test_y,Y_pred_rf))


from sklearn.feature_selection import SelectFromModel

selector = SelectFromModel(RandomForestRegressor(n_estimators=100))
selector.fit(train_X, train_y)

# 获取重要特征
train_X_selected = selector.transform(train_X)
test_X_selected = selector.transform(test_X)


rf_regr = RandomForestRegressor()
rf_regr.fit(train_X_selected, train_y)
Y_pred_training_rf = rf_regr.predict(train_X_selected)
Y_pred_rf = rf_regr.predict(test_X_selected)

# MAPE metric
print(mean_absolute_percentage_error(train_y,Y_pred_training_rf))
print(mean_absolute_percentage_error(test_y,Y_pred_rf))


train_X = X_pca[:num_train_samples]
test_X = X_pca[num_train_samples:]

rf_regr.fit(train_X, train_y)
Y_pred_training_rf = rf_regr.predict(train_X)
Y_pred_rf = rf_regr.predict(test_X)

# MAPE metric
print(mean_absolute_percentage_error(train_y,Y_pred_training_rf))
print(mean_absolute_percentage_error(test_y,Y_pred_rf))



